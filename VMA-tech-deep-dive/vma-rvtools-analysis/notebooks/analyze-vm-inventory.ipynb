{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa7e99f-0c30-40e7-be7c-37f29aa9007f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Python 3 notebook explores the exported VMWare inventory generated by [RVTools](https://www.robware.net/) and generates various statistical analyses of the data for assessing the scale, complexity, and other characteristics.\n",
    "\n",
    "The analysis is directed towards understanding the feasibility of migrating these VMWare virtual machines to Red Hat's OpenShift Virtualization Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e010991-ca96-43de-af5f-cd22bfdbd72e",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "\n",
    "This section configures the script and the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Required Packages\n",
    "\n",
    "This script uses `pandas` and `numpy` for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the VM inventory generated by rvtools\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Set up a few options and other configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to prevent line wrapping\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Set up the data directory\n",
    "First, set up the directory where the RVTools Excel data files will be stored. This folder must also contain an index.xlsx file (refer to the provided template for the expected format). The index.xlsx file should list the valid RVTools Excel file names along with their corresponding vCenter instances. This script assumes that there is one Excel file per vCenter instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current script directory\n",
    "current_dir = \".\"\n",
    "\n",
    "# Specify the directory containing the Excel files\n",
    "DATA_DIR = os.path.join(current_dir, '../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and configuration parameters\n",
    "\n",
    "# The index file\n",
    "INDEX_FILENAME = \"index.xlsx\"\n",
    "INDEX_FILEPATH = os.path.join(DATA_DIR, INDEX_FILENAME)\n",
    "INDEX_SHEETNAME = \"index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "This section defines a few functions for decomposing the analysis code. It reads multiple Excel .xlsx files exported from RVtools software and returns a dictionary with filenames as keys and a dictionary of DataFrames (one for each sheet) as values.\n",
    "\n",
    "* The `index.xlsx` and `index_template.xlsx` files are explicitly ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rvtools_excel_files(directory, filenames_to_process):\n",
    "    rvtools_data = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        # Only process files listed in the index file.\n",
    "        filename_base, _ = os.path.splitext(filename)\n",
    "        if filename_base not in filenames_to_process: continue\n",
    "        if filename in ['index.xlsx', 'index_template.xlsx']: continue\n",
    "\n",
    "        if filename.endswith('.xlsx') or filename.endswith('.xls'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            excel_data = pd.read_excel(filepath, sheet_name=None)\n",
    "            rvtools_data[filename] = excel_data\n",
    "\n",
    "    return rvtools_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Read, Clean, and Filter the RVTools data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Read the index metadata file\n",
    "\n",
    "First read the `index.xlsx` file to determine the RVTools files to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First read the index Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Read the index Excel file\n",
    "index_df = pd.read_excel(\n",
    "    INDEX_FILEPATH, sheet_name=INDEX_SHEETNAME, \n",
    "    nrows=19, index_col='vCenter', \n",
    "    true_values=['Yes', 'Y'], false_values=['No', 'N'], \n",
    "    na_filter=False, dtype={'In Scope': bool}\n",
    ")\n",
    "\n",
    "# Clean up column names and handle missing values\n",
    "index_df.columns = index_df.columns.str.replace(' ', '_')\n",
    "index_df.fillna('', inplace=True)\n",
    "\n",
    "# Filter for in-scope vCenters\n",
    "inscope_df = index_df[index_df['In_Scope']].reset_index()\n",
    "\n",
    "# Count occurrences of each vCenter\n",
    "pivot_table = inscope_df.groupby('vCenter').size().reset_index(name='Count')\n",
    "\n",
    "# Print in a structured table format\n",
    "print(\"üîç In-Scope vCenter Instances:\")\n",
    "print(pivot_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00709fbf-13ea-49e8-af60-8fac524d0244",
   "metadata": {},
   "source": [
    "### Read the RVTool Exported Spreadsheets\n",
    "\n",
    "Read all RVTools exported files available in the `data` directory. The data will be stored in a dictionary, where each filename serves as a key. The corresponding value is a nested dictionary, where sheet names act as keys, and each value is a `DataFrame` containing the sheet's data.\n",
    "\n",
    "**Note**: This step may take a few minutes to complete. Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fce93b-3805-4903-becf-43b2fc78bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read the index Excel file\n",
    "index_df = pd.read_excel(\n",
    "    INDEX_FILEPATH, sheet_name=INDEX_SHEETNAME, \n",
    "    nrows=19, index_col='vCenter', \n",
    "    true_values=['Yes', 'Y'], false_values=['No', 'N'], \n",
    "    na_filter=False, dtype={'In Scope': bool}\n",
    ")\n",
    "\n",
    "# Clean up column names and handle missing values\n",
    "index_df.columns = index_df.columns.str.replace(' ', '_')\n",
    "index_df.fillna('', inplace=True)\n",
    "\n",
    "# Ensure 'In_Scope' column is boolean\n",
    "index_df['In_Scope'] = index_df['In_Scope'].astype(bool)\n",
    "\n",
    "# Filter for in-scope vCenters\n",
    "inscope_df = index_df[index_df['In_Scope']].reset_index()\n",
    "\n",
    "# Extract list of in-scope vCenter instances\n",
    "inscope_vcenter_instances = set(inscope_df['vCenter'].tolist())  # Use a set for faster lookups\n",
    "\n",
    "# Function to read RVTools Excel files while excluding 'index_template.xlsx' and 'index.xlsx'\n",
    "def read_rvtools_excel_files(directory, vcenters):\n",
    "    rvtools_data = {}\n",
    "    exclude_files = {\"index_template.xlsx\", \"index.xlsx\"}  # Set of filenames to exclude\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".xlsx\") and file.lower() not in exclude_files:\n",
    "            vcenter_match = next((vc for vc in vcenters if vc in file), None)  # Check if file contains an in-scope vCenter\n",
    "            if vcenter_match:\n",
    "                file_path = os.path.join(directory, file)\n",
    "                try:\n",
    "                    xls = pd.ExcelFile(file_path)\n",
    "                    sheets = {sheet: xls.parse(sheet) for sheet in xls.sheet_names}\n",
    "                    rvtools_data[file] = sheets\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    return rvtools_data\n",
    "\n",
    "# Read the RVTools Excel files\n",
    "rvtools_data = read_rvtools_excel_files(\"../data\", inscope_vcenter_instances)\n",
    "\n",
    "# Display the loaded data (for demonstration purposes)\n",
    "for filename, sheets in rvtools_data.items():\n",
    "    print(f'Processed RVTools File: {filename}')\n",
    "    for sheet_name, df in sheets.items():\n",
    "        print(f\"  Sheet: {sheet_name} Shape: {df.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖÔ∏è \"f'Total files processed: {len(rvtools_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Create the consolidated data frames\n",
    "\n",
    "Create two consolidated dataframes containing information from all `vCenter` instances:\n",
    "\n",
    "1. One dataframe containing all the `vInfo` (VM's) details\n",
    "2. The second one containing all the `vHost` (HW) details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize the lists\n",
    "vinfo_sheets = []\n",
    "vhost_sheets = []\n",
    "vdisk_sheets = []\n",
    "\n",
    "# Load vInfo, vHost and vDisk sheets into the lists\n",
    "for filename, sheets in rvtools_data.items():\n",
    "    vCenter = filename.split('.')[0].lower()  # Extract vCenter instance name from the filename\n",
    "\n",
    "    # Ensure required sheets exist before processing\n",
    "    if 'vInfo' in sheets and 'vHost' in sheets and 'vDisk' in sheets:\n",
    "        # Add the vCenter instance name to each sheet\n",
    "        sheets['vInfo']['vCenter'] = vCenter\n",
    "        sheets['vHost']['vCenter'] = vCenter\n",
    "        sheets['vDisk']['vCenter'] = vCenter\n",
    "\n",
    "        # Append to the respective lists\n",
    "        vinfo_sheets.append(sheets['vInfo'])\n",
    "        vhost_sheets.append(sheets['vHost'])\n",
    "        vdisk_sheets.append(sheets['vDisk'])\n",
    "\n",
    "# Filter out empty or all-NA DataFrames\n",
    "vinfo_sheets = [df for df in vinfo_sheets if not df.dropna(how='all').empty]\n",
    "vhost_sheets = [df for df in vhost_sheets if not df.dropna(how='all').empty]\n",
    "vdisk_sheets = [df for df in vdisk_sheets if not df.dropna(how='all').empty]\n",
    "\n",
    "# Concatenate the filtered DataFrames only if there are valid sheets\n",
    "consolidated_vinfo_df = pd.concat(vinfo_sheets, ignore_index=True) if vinfo_sheets else pd.DataFrame()\n",
    "consolidated_vhost_df = pd.concat(vhost_sheets, ignore_index=True) if vhost_sheets else pd.DataFrame()\n",
    "consolidated_vdisk_df = pd.concat(vdisk_sheets, ignore_index=True) if vdisk_sheets else pd.DataFrame()\n",
    "\n",
    "# Verify data ingestion success with improved readability\n",
    "print(\"üîç Data Ingestion Verification\")\n",
    "print(\"\\n‚úÖÔ∏è \"f\"Total vInfo (Total VM's) records: {len(consolidated_vinfo_df)}\")\n",
    "print(\"‚úÖÔ∏è \"f\"Total vHost (Total HW) records: {len(consolidated_vhost_df)}\")\n",
    "print(\"‚úÖÔ∏è \"f\"Total vDisk (Total VM Disks) records: {len(consolidated_vdisk_df)}\\n\")\n",
    "\n",
    "if not consolidated_vinfo_df.empty:\n",
    "    print(\"üìÑ Sample vInfo Data:\")\n",
    "    print(consolidated_vinfo_df.head(3).to_string(index=False), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fe759-4222-4227-a0e1-896b7e60de1b",
   "metadata": {},
   "source": [
    "### Distribution of VMs per In-Scope vCenter\n",
    "\n",
    "What is the total percentage of VMs per vCenter? This includes the total VM count, accounting for VM templates, SRM placeholders, powered-off VMs, and orphaned objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure that consolidated_vinfo_df exists before proceeding\n",
    "try:\n",
    "    # Group by vCenter and count VMs\n",
    "    grouped_summary = consolidated_vinfo_df.groupby(\"vCenter\").size().reset_index(name=\"Count\")\n",
    "\n",
    "    # Create a pivot table for better visualization\n",
    "    grouped_summary_pivot = grouped_summary.pivot_table(values=\"Count\", index=\"vCenter\", aggfunc=\"sum\")\n",
    "\n",
    "    # Print summary\n",
    "    total_vms = grouped_summary[\"Count\"].sum()\n",
    "    total_vcenters = grouped_summary.shape[0]\n",
    "\n",
    "    print(f\"üîç Overall Distribution of {total_vms:,} VMs in the {total_vcenters:,} vCenter instances:\")\n",
    "    print(grouped_summary_pivot)\n",
    "    print(\"\\nüìù This is the TOTAL VM count, and WILL include VM templates, SRM Placeholders, Powered Off & Orphaned Objects...\")\n",
    "\n",
    "    # Calculate original percentage distribution\n",
    "    percentages = grouped_summary[\"Count\"] / total_vms * 100\n",
    "\n",
    "    # Identify slices below .5% and adjust to at least 1%\n",
    "    min_threshold = 0.5\n",
    "    new_min_threshold = 1\n",
    "    adjusted_percentages = percentages.copy()\n",
    "\n",
    "    # Find slices below threshold\n",
    "    below_threshold_mask = adjusted_percentages < min_threshold\n",
    "    num_below_threshold = below_threshold_mask.sum()\n",
    "    \n",
    "    if num_below_threshold > 0:\n",
    "        # Calculate deficit caused by increasing small slices to 10%\n",
    "        deficit = new_min_threshold * num_below_threshold - adjusted_percentages[below_threshold_mask].sum()\n",
    "\n",
    "        # Increase small slices to 10%\n",
    "        adjusted_percentages[below_threshold_mask] = new_min_threshold\n",
    "\n",
    "        # Reduce larger slices proportionally to compensate for the increase\n",
    "        large_slices_mask = ~below_threshold_mask  # Slices that are >= min_threshold\n",
    "        if large_slices_mask.sum() > 0:\n",
    "            scale_factor = (100 - new_min_threshold * num_below_threshold) / adjusted_percentages[large_slices_mask].sum()\n",
    "            adjusted_percentages[large_slices_mask] *= scale_factor\n",
    "\n",
    "    # Ensure sum is exactly 100% (fix potential floating-point issues)\n",
    "    adjusted_percentages = adjusted_percentages.round(1)\n",
    "    adjusted_percentages.iloc[-1] += 100 - adjusted_percentages.sum()  # Correct final rounding error safely\n",
    "\n",
    "    # Generate explode values to emphasize each slice\n",
    "    explode_values = [0.05] * len(grouped_summary)  # Adjust explosion for all slices\n",
    "\n",
    "    # Get colors from the tab20 colormap\n",
    "    cmap = plt.get_cmap(\"tab20\")\n",
    "    colors = [cmap(i % 10) for i in range(len(grouped_summary))]  # Cycle through the colormap\n",
    "\n",
    "    # Plot an exploded pie chart with adjusted percentages\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(\n",
    "        adjusted_percentages,\n",
    "        labels=grouped_summary[\"vCenter\"],\n",
    "        autopct=lambda p: f\"{p:.1f}%\",  # Ensures every slice displays a percentage\n",
    "        startangle=140,\n",
    "        explode=explode_values,  # Add explode effect\n",
    "        shadow=True,  # Add shadow for better visualization\n",
    "        colors=colors  # Use tab20 colors\n",
    "    )\n",
    "    plt.title(\"\\nDistribution of VMs by vCenter Instances\")\n",
    "    plt.show()\n",
    "\n",
    "except NameError:\n",
    "    print(\"Error: The dataset 'consolidated_vinfo_df' is not defined.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Clean the VM List\n",
    "Filter the consolidated `vInfo` content using the following criteria:\n",
    "\n",
    "1. Remove all `template` entries.\n",
    "2. Remove all `SRM Placeholder` entries.\n",
    "3. Remove all `orphaned VM object` entries.\n",
    "4. Remove all `other objects` entries.\n",
    "\n",
    "The cleaned results for `In-Scope VMs` and `Out-of-Scope VMs` are saved in the `helper_files` directory. These files will be used later for analyzing different sections.\n",
    "\n",
    "To modify `ignore_patterns` and `os_filter_patterns`, refer to the **README.md** file located in the `helper_files` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61863a1-bd86-445c-b261-d22d12c13cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Function to load patterns from an external file\n",
    "def load_patterns(file_path):\n",
    "    \"\"\"Reads patterns from an external file.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        patterns = [line.strip() for line in file if line.strip()]\n",
    "    return patterns\n",
    "\n",
    "# Load OS filter patterns and ignore patterns\n",
    "os_filter_patterns = load_patterns(\"../helper_files/os_filter_patterns.txt\")\n",
    "ignore_patterns = load_patterns(\"../helper_files/ignored_patterns.txt\")\n",
    "\n",
    "def clean_os_name(os_name):\n",
    "    \"\"\"Normalize OS names by removing extra spaces and redundant information.\"\"\"\n",
    "    if pd.isna(os_name) or os_name.strip() == '':\n",
    "        return ''\n",
    "    os_name = os_name.strip()\n",
    "    os_name = re.sub(r\"\\s*\\(.*\\)$\", \"\", os_name)  # Remove (32-bit) / (64-bit)\n",
    "    os_name = re.sub(r\"\\s+\", \" \", os_name)  # Remove extra spaces\n",
    "    return os_name\n",
    "\n",
    "def os_filter(os_name):\n",
    "    \"\"\"Check if the OS should be filtered based on the defined patterns.\"\"\"\n",
    "    if pd.isna(os_name) or os_name.strip() == '':\n",
    "        return False\n",
    "    os_name_clean = clean_os_name(os_name)\n",
    "    return any(re.fullmatch(pattern, os_name_clean, re.IGNORECASE) for pattern in os_filter_patterns)\n",
    "\n",
    "# Ensure the DataFrame exists before processing\n",
    "if 'consolidated_vinfo_df' not in globals():\n",
    "    raise ValueError(\"consolidated_vinfo_df is not defined.\")\n",
    "if 'consolidated_vdisk_df' not in globals():\n",
    "    raise ValueError(\"consolidated_vdisk_df is not defined.\")\n",
    "\n",
    "# Ensure OS column is populated, tagging Unknown if both sources are empty\n",
    "consolidated_vinfo_df['OS Effective'] = consolidated_vinfo_df['OS according to the VMware Tools'].fillna(\n",
    "    consolidated_vinfo_df['OS according to the configuration file']\n",
    ")\n",
    "consolidated_vinfo_df.loc[\n",
    "    consolidated_vinfo_df['OS Effective'].isna() |\n",
    "    (consolidated_vinfo_df['OS Effective'].str.strip() == ''),\n",
    "    'OS Effective'\n",
    "] = 'Unknown'\n",
    "\n",
    "# Normalize OS names before filtering\n",
    "consolidated_vinfo_df['Cleaned OS'] = consolidated_vinfo_df['OS Effective'].apply(clean_os_name)\n",
    "\n",
    "# Initialize exclusion reason column\n",
    "consolidated_vinfo_df['Exclusion Reason'] = ''\n",
    "\n",
    "# Apply exclusion rules using escaped ignore patterns\n",
    "consolidated_vinfo_df.loc[\n",
    "    consolidated_vinfo_df['VM'].str.contains('|'.join(map(re.escape, ignore_patterns)), case=False, na=False),\n",
    "    'Exclusion Reason'\n",
    "] = 'Ignored VM Pattern'\n",
    "\n",
    "consolidated_vinfo_df.loc[\n",
    "    consolidated_vinfo_df['Cleaned OS'].apply(os_filter),\n",
    "    'Exclusion Reason'\n",
    "] = 'Excluded OS'\n",
    "\n",
    "# Exclude 'Unknown' OS VMs\n",
    "consolidated_vinfo_df.loc[\n",
    "    consolidated_vinfo_df['OS Effective'] == 'Unknown',\n",
    "    'Exclusion Reason'\n",
    "] = 'Unknown OS'\n",
    "\n",
    "# Ensure 'Template' column is properly processed\n",
    "consolidated_vinfo_df['Template'] = consolidated_vinfo_df['Template'].fillna(False).astype(bool)\n",
    "consolidated_vinfo_df.loc[\n",
    "    consolidated_vinfo_df['Template'],\n",
    "    'Exclusion Reason'\n",
    "] = 'Template'\n",
    "\n",
    "consolidated_vinfo_df.loc[\n",
    "    consolidated_vinfo_df['SRM Placeholder'].fillna(False) == True,\n",
    "    'Exclusion Reason'\n",
    "] = 'SRM Placeholder'\n",
    "\n",
    "consolidated_vinfo_df.loc[\n",
    "    consolidated_vinfo_df['Connection state'].fillna('').str.lower() == 'orphaned',\n",
    "    'Exclusion Reason'\n",
    "] = 'Orphaned VM'\n",
    "\n",
    "# Powered off VMs with excluded OS\n",
    "consolidated_vinfo_df.loc[\n",
    "    (consolidated_vinfo_df['Powerstate'].fillna('').str.lower() == 'poweredoff') &\n",
    "    (consolidated_vinfo_df['Cleaned OS'].apply(os_filter)),\n",
    "    'Exclusion Reason'\n",
    "] = 'Powered Off'\n",
    "\n",
    "# ‚úÖ Controller-based filtering using vDisk sheet\n",
    "if 'Controller' in consolidated_vdisk_df.columns:\n",
    "    controller_pattern = r'i440fx|ide\\s*\\d+'\n",
    "    matching_vms = consolidated_vdisk_df[\n",
    "        consolidated_vdisk_df['Controller'].str.contains(controller_pattern, case=False, na=False, regex=True)\n",
    "    ]['VM'].unique()\n",
    "\n",
    "    consolidated_vinfo_df.loc[\n",
    "        (consolidated_vinfo_df['VM'].isin(matching_vms)) &\n",
    "        (consolidated_vinfo_df['Exclusion Reason'] == ''),\n",
    "        'Exclusion Reason'\n",
    "    ] = 'i440fx or IDE Controller'\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è The 'Controller' column does not exist in the consolidated_vdisk_df.\")\n",
    "\n",
    "# Filter in-scope VMs\n",
    "filtered_vinfo_df = consolidated_vinfo_df[consolidated_vinfo_df['Exclusion Reason'] == '']\n",
    "\n",
    "# Debugging: Ensure no Templates exist in filtered VMs\n",
    "assert not filtered_vinfo_df['Template'].any(), \"Templates are still present in In-Scope VMs!\"\n",
    "\n",
    "# Categorize ignored VMs\n",
    "ignored_vm_artifacts = consolidated_vinfo_df[consolidated_vinfo_df['Exclusion Reason'] == 'Excluded OS']\n",
    "ignored_artifacts = consolidated_vinfo_df[\n",
    "    consolidated_vinfo_df['Exclusion Reason'].isin(['Template', 'SRM Placeholder', 'Orphaned VM', 'Unknown OS'])\n",
    "]\n",
    "ignored_controllers = consolidated_vinfo_df[\n",
    "    consolidated_vinfo_df['Exclusion Reason'] == 'i440fx or IDE Controller'\n",
    "]\n",
    "\n",
    "# Save data to CSV files\n",
    "filtered_vinfo_df.to_csv(\"../saved_csv_files/In_Scope_VMs.csv\", index=False)\n",
    "ignored_vm_artifacts.to_csv(\"../saved_csv_files/Out_of_Scope_VMs.csv\", index=False)\n",
    "ignored_controllers.to_csv(\"../saved_csv_files/Unsupported_disk_Controllers.csv\", index=False)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüîç Filtering Summary\")\n",
    "print(f\"üî∂ Out-of-Scope VM count (OS): {len(ignored_vm_artifacts):,}\")\n",
    "print(f\"üî∂ Ignored Artifacts (Template, SRM, Orphaned, Unknown OS): {len(ignored_artifacts):,}\")\n",
    "print(f\"üî∂ Ignored Controllers: {len(ignored_controllers):,}\")\n",
    "print(f\"üî∑ In-Scope VM count: {len(filtered_vinfo_df):,}\\n\")\n",
    "\n",
    "# Pie Chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(\n",
    "    [len(ignored_vm_artifacts), len(ignored_artifacts), len(ignored_controllers), len(filtered_vinfo_df)],\n",
    "    labels=['Out-of-Scope VMs', 'Ignored Artifacts', 'Unsupported Controllers', 'In-Scope VMs'],\n",
    "    autopct='%1.1f%%', shadow=True, startangle=140,\n",
    "    explode=(0.05, 0.05, 0.05, 0),\n",
    "    colors=['lightcoral', 'lightskyblue', 'red', 'lightgreen']\n",
    ")\n",
    "plt.title(\"\\nBreakdown of VM Filtering\")\n",
    "plt.show()\n",
    "\n",
    "# Confirm file creation\n",
    "print(\"\\n‚úÖ CSV files saved successfully to ../saved_csv_files:\")\n",
    "print(\"üìÅ In-Scope VMs: 'In_Scope_VMs.csv'\")\n",
    "print(\"üìÅ Out-of-Scope VMs: 'Out_of_Scope_VMs.csv'\")\n",
    "print(\"üìÅ Unsupported Controllers: 'Unsupported_disk_Controllers.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Summary of vInfo & vHosts after cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Select the in-scope vCenter instances\n",
    "inscope_vinfo_condition = filtered_vinfo_df['vCenter'].isin(inscope_vcenter_instances)\n",
    "inscope_vinfo_df = filtered_vinfo_df[inscope_vinfo_condition]\n",
    "\n",
    "# Summary stats for in-scope VMs\n",
    "total_vm_count = len(filtered_vinfo_df)\n",
    "inscope_vm_count = len(inscope_vinfo_df)\n",
    "percent_inscope_vms = (inscope_vm_count / total_vm_count * 100.0) if total_vm_count > 0 else 0.0\n",
    "\n",
    "print(\"\\nüîç VM (vInfo) Summary\")\n",
    "print(f\"‚úÖ {inscope_vm_count:,} VMs are in-scope ({percent_inscope_vms:0.2f}% of total VMs).\\n\")\n",
    "\n",
    "# Create a pivot table for VMs by vCenter\n",
    "vm_pivot = inscope_vinfo_df.pivot_table(index=\"vCenter\", aggfunc=\"size\").reset_index()\n",
    "vm_pivot.columns = [\"vCenter\", \"VM Count\"]\n",
    "display(vm_pivot)\n",
    "\n",
    "# Select the in-scope hosts\n",
    "inscope_vhost_condition = consolidated_vhost_df['vCenter'].isin(inscope_vcenter_instances)\n",
    "inscope_vhost_df = consolidated_vhost_df[inscope_vhost_condition]\n",
    "\n",
    "# Summary stats for in-scope hosts\n",
    "total_host_count = len(consolidated_vhost_df)\n",
    "inscope_host_count = len(inscope_vhost_df)\n",
    "percent_inscope_hosts = (inscope_host_count / total_host_count * 100.0) if total_host_count > 0 else 0.0\n",
    "\n",
    "print(\"\\nüîç Host (vHost) Summary\")\n",
    "print(f\"‚úÖ {inscope_host_count:,} hosts are in-scope ({percent_inscope_hosts:0.2f}% of total hosts).\\n\")\n",
    "\n",
    "# Create a pivot table for hosts by vCenter\n",
    "host_pivot = inscope_vhost_df.pivot_table(index=\"vCenter\", aggfunc=\"size\").reset_index()\n",
    "host_pivot.columns = [\"vCenter\", \"Host Count\"]\n",
    "display(host_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb6031-4de5-4f22-937a-ece58b28b701",
   "metadata": {},
   "source": [
    "# ========== Analysis =========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 1- Create a consolidated view of the client's landscape\n",
    "\n",
    "Summary of VM's (vCPU, Memory, etc) & Host's (CPU, Cores, etc)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === vInfo Pivot Table (VM-Level Info) ===\n",
    "vinfo_pivot_df = filtered_vinfo_df.pivot_table(\n",
    "    index='vCenter',\n",
    "    values=['VM', 'CPUs', 'Memory', 'NICs', 'Provisioned MiB'],\n",
    "    aggfunc={\n",
    "        'VM': 'count',\n",
    "        'CPUs': 'sum',\n",
    "        'Memory': 'sum',\n",
    "        'NICs': 'sum',\n",
    "        'Provisioned MiB': 'sum'\n",
    "    },\n",
    "    margins=False\n",
    ")\n",
    "\n",
    "# === vHost Pivot Table (Host-Level Info) ===\n",
    "vhost_pivot_df = consolidated_vhost_df.pivot_table(\n",
    "    index='vCenter',\n",
    "    values=['Host', '# VMs total', '# CPU', '# Memory', '# Cores'],\n",
    "    aggfunc={\n",
    "        'Host': 'count',\n",
    "        '# CPU': 'sum',\n",
    "        '# Memory': 'sum',\n",
    "        '# Cores': 'sum'\n",
    "    },\n",
    "    margins=False\n",
    ")\n",
    "\n",
    "# === Convert Memory to GB/TB ===\n",
    "def format_storage(mib_value):\n",
    "    if pd.isna(mib_value):\n",
    "        return \"N/A\"\n",
    "    if mib_value >= 1_000_000:  # Convert to TB if ‚â• 1,000,000 MiB\n",
    "        return f\"{mib_value / 1_048_576:.2f} TB\"\n",
    "    return f\"{mib_value / 1024:.2f} GB\"  # Convert to GB otherwise\n",
    "\n",
    "# --- DEBUGGING STEP (optional) ---\n",
    "# print(\"vinfo_pivot_df columns:\", vinfo_pivot_df.columns)\n",
    "\n",
    "# Check if 'Provisioned MiB' exists before applying\n",
    "if 'Provisioned MiB' in vinfo_pivot_df.columns:\n",
    "    vinfo_pivot_df['Total Disk Capacity'] = vinfo_pivot_df['Provisioned MiB'].apply(format_storage)\n",
    "    vinfo_pivot_df = vinfo_pivot_df.drop(columns=['Provisioned MiB'])\n",
    "else:\n",
    "    vinfo_pivot_df['Total Disk Capacity'] = \"N/A\"\n",
    "\n",
    "# Apply formatting to Memory\n",
    "if 'Memory' in vinfo_pivot_df.columns:\n",
    "    vinfo_pivot_df['Memory'] = vinfo_pivot_df['Memory'].apply(format_storage)\n",
    "\n",
    "if '# Memory' in vhost_pivot_df.columns:\n",
    "    vhost_pivot_df['# Memory'] = vhost_pivot_df['# Memory'].apply(format_storage)\n",
    "\n",
    "# === Pretty Display for Each Table ===\n",
    "try:\n",
    "    from IPython.display import display  # Works for Jupyter Notebook\n",
    "\n",
    "    print(\"\\nüîç VM Info (vInfo)\")\n",
    "    display(vinfo_pivot_df)\n",
    "\n",
    "    print(\"\\nüîç Host Info (vHost)\")\n",
    "    display(vhost_pivot_df)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nüîç VM Info (vInfo)\")\n",
    "    print(vinfo_pivot_df)\n",
    "\n",
    "    print(\"\\nüîç Host Info (vHost)\")\n",
    "    print(vhost_pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### 2- Summarize the Operating Systems\n",
    "\n",
    "Provide a breakdown of `In-Scope` VMs based on their operating systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the data\n",
    "in_scope_df = pd.read_csv(\"../saved_csv_files/In_Scope_VMs.csv\", low_memory=False)\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "required_columns = {'Cleaned OS', 'VM'}\n",
    "if not required_columns.issubset(in_scope_df.columns):\n",
    "    raise ValueError(f\"Missing required columns: {required_columns - set(in_scope_df.columns)}\")\n",
    "\n",
    "# Drop rows where VM or Cleaned OS is missing\n",
    "in_scope_df = in_scope_df.dropna(subset=['VM', 'Cleaned OS'])\n",
    "\n",
    "# If DataFrame is empty after cleaning\n",
    "if in_scope_df.empty:\n",
    "    print(\"Warning: No VMs are in scope or input data is empty.\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 2))\n",
    "    ax.text(0.5, 0.5, \"No VMs are in scope\", fontsize=14, ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "    plt.show()\n",
    "else:\n",
    "    inscope_guest_os_pivot = in_scope_df.pivot_table(index='Cleaned OS', values='VM', aggfunc='count')\n",
    "    inscope_guest_os_pivot = inscope_guest_os_pivot.sort_values(by='VM', ascending=True)\n",
    "\n",
    "    if inscope_guest_os_pivot.empty:\n",
    "        print(\"Warning: The pivot table is empty after filtering.\")\n",
    "        fig, ax = plt.subplots(figsize=(10, 2))\n",
    "        ax.text(0.5, 0.5, \"No VMs are in scope\", fontsize=14, ha='center', va='center')\n",
    "        ax.axis('off')\n",
    "        plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "        plt.show()\n",
    "    else:\n",
    "        total_vms = inscope_guest_os_pivot['VM'].sum()\n",
    "        percentages = (inscope_guest_os_pivot['VM'] / total_vms * 100).round(1)\n",
    "\n",
    "        fig_height = max(6, len(inscope_guest_os_pivot) * 0.4)\n",
    "        fig, ax = plt.subplots(figsize=(16, fig_height))  # Wider figure to avoid label cutoff\n",
    "\n",
    "        bars = ax.barh(inscope_guest_os_pivot.index, inscope_guest_os_pivot['VM'], color=plt.cm.tab20.colors)\n",
    "\n",
    "        for bar, count, percentage in zip(bars, inscope_guest_os_pivot['VM'], percentages):\n",
    "            ax.text(bar.get_width() + 2, bar.get_y() + bar.get_height()/2,\n",
    "                    f\"{count} VMs ({percentage}%)\", va='center', fontsize=8, color='black')\n",
    "\n",
    "        ax.set_xlabel(\"Number of VMs\", fontsize=10)\n",
    "        ax.set_ylabel(\"Operating Systems\", fontsize=10)\n",
    "        ax.set_title(\"OS Distribution\", fontsize=12, pad=20)\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "        # Manually adjust margins\n",
    "        plt.subplots_adjust(left=0.3, right=0.95, top=0.9, bottom=0.1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 3- Operating Systems with Over 500 Associated VMs\n",
    "\n",
    "This section summarizes guest operating systems that have more than 500 associated VMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Load CSV\n",
    "in_scope_df = pd.read_csv(\"../saved_csv_files/In_Scope_VMs.csv\", low_memory=False)\n",
    "\n",
    "# Check required columns\n",
    "required_columns = {'Cleaned OS', 'VM'}\n",
    "if not required_columns.issubset(in_scope_df.columns):\n",
    "    raise ValueError(f\"Missing required columns: {required_columns - set(in_scope_df.columns)}\")\n",
    "\n",
    "# Drop rows with missing values in key columns\n",
    "in_scope_df = in_scope_df.dropna(subset=['VM', 'Cleaned OS'])\n",
    "\n",
    "# Ensure 'VM' is treated as a string (or any type that works well)\n",
    "in_scope_df['VM'] = in_scope_df['VM'].astype(str)\n",
    "\n",
    "# Group and count VMs per OS (equivalent to pivot with count)\n",
    "inscope_guest_os_pivot = (\n",
    "    in_scope_df.groupby('Cleaned OS')\n",
    "    .size()\n",
    "    .reset_index(name='VM_Count')\n",
    "    .sort_values(by='VM_Count', ascending=False)\n",
    "    .set_index('Cleaned OS')\n",
    ")\n",
    "\n",
    "# Filter for OSs with over 500 VMs\n",
    "over_500_vms = inscope_guest_os_pivot[inscope_guest_os_pivot['VM_Count'] > 500]\n",
    "\n",
    "# Display the table\n",
    "display(over_500_vms)\n",
    "\n",
    "# Plot pie chart\n",
    "if over_500_vms.empty:\n",
    "    print(\"‚ö†Ô∏è No operating systems have more than 500 VMs.\")\n",
    "else:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    colors = plt.cm.tab20.colors[:len(over_500_vms)]\n",
    "    explode = [0.05] * len(over_500_vms)\n",
    "\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        over_500_vms['VM_Count'],\n",
    "        labels=over_500_vms.index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=140,\n",
    "        colors=colors,\n",
    "        explode=explode,\n",
    "        shadow=True,\n",
    "    )\n",
    "\n",
    "    for text in texts:\n",
    "        text.set_fontsize(10)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(10)\n",
    "        autotext.set_color('white')\n",
    "\n",
    "    plt.title(\"In-Scope OS's with Over 500 VMs\", fontsize=12, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0133b5-3674-4a1f-9034-6f7433e70acc",
   "metadata": {},
   "source": [
    "### 4- Group VMs by Memory Size\n",
    "\n",
    "In this section, we group the VMs according to their memory size tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define memory tiers (in GB) in ascending order\n",
    "memory_tiers = {\n",
    "    '0-4 GB': (0, 4),\n",
    "    '4-16 GB': (4, 16),\n",
    "    '16-32 GB': (16, 32),\n",
    "    '32-64 GB': (32, 64),\n",
    "    '64-128 GB': (64, 128),\n",
    "    '128-256 GB': (128, 256),\n",
    "    '256+ GB': (256, float('inf'))\n",
    "}\n",
    "memory_tier_order = list(memory_tiers.keys())  # Preserve tier order for sorting\n",
    "\n",
    "# Function to categorize memory\n",
    "def categorize_memory(memory_gb):\n",
    "    for tier, (lower, upper) in memory_tiers.items():\n",
    "        if lower <= memory_gb < upper:\n",
    "            return tier\n",
    "    return 'Unknown'\n",
    "\n",
    "# Load the data\n",
    "in_scope_df = pd.read_csv(\"../saved_csv_files/In_Scope_VMs.csv\", low_memory=False)\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "required_columns = {'Cleaned OS', 'VM', 'Memory'}\n",
    "missing_columns = required_columns - set(in_scope_df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# Convert memory to numeric and to GB\n",
    "in_scope_df['Memory'] = pd.to_numeric(in_scope_df['Memory'], errors='coerce')\n",
    "in_scope_df.dropna(subset=['Memory'], inplace=True)\n",
    "in_scope_df['Memory GB'] = in_scope_df['Memory'] / 1024\n",
    "\n",
    "# Apply memory tier categorization\n",
    "in_scope_df['Memory Tier'] = in_scope_df['Memory GB'].apply(categorize_memory)\n",
    "\n",
    "# Convert Memory Tier to categorical with correct order\n",
    "in_scope_df['Memory Tier'] = pd.Categorical(in_scope_df['Memory Tier'], categories=memory_tier_order, ordered=True)\n",
    "\n",
    "# Create summary table\n",
    "memory_tier_summary = (\n",
    "    in_scope_df.groupby('Memory Tier', observed=True)\n",
    "    .agg({'VM': 'count'})\n",
    "    .rename(columns={'VM': 'VM Count'})\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Remove tiers with zero VMs\n",
    "memory_tier_summary = memory_tier_summary[memory_tier_summary['VM Count'] > 0]\n",
    "\n",
    "# Handle empty case\n",
    "if memory_tier_summary.empty:\n",
    "    print(\"‚ö†Ô∏è Warning: No VMs are in-scope. Memory Tier Summary is empty.\")\n",
    "else:\n",
    "    # Calculate original percentages\n",
    "    total_vms = memory_tier_summary['VM Count'].sum()\n",
    "    original_percentages = (memory_tier_summary['VM Count'] / total_vms) * 100\n",
    "\n",
    "    # Adjust small slices\n",
    "    min_percentage = 0.5\n",
    "    new_min_threshold = 1\n",
    "    adjusted_percentages = original_percentages.copy()\n",
    "\n",
    "    below_threshold_mask = adjusted_percentages < min_percentage\n",
    "    num_below_threshold = below_threshold_mask.sum()\n",
    "\n",
    "    if num_below_threshold > 0:\n",
    "        deficit = new_min_threshold * num_below_threshold - adjusted_percentages[below_threshold_mask].sum()\n",
    "        adjusted_percentages[below_threshold_mask] = new_min_threshold\n",
    "        large_slices_mask = ~below_threshold_mask\n",
    "        if large_slices_mask.sum() > 0:\n",
    "            scale_factor = (100 - new_min_threshold * num_below_threshold) / adjusted_percentages[large_slices_mask].sum()\n",
    "            adjusted_percentages[large_slices_mask] *= scale_factor\n",
    "\n",
    "    # Normalize to 100%\n",
    "    adjusted_percentages = adjusted_percentages.round(1)\n",
    "    if len(adjusted_percentages) > 0:\n",
    "        adjusted_percentages.iloc[-1] += 100 - adjusted_percentages.sum()\n",
    "\n",
    "    # Formatter for pie chart labels\n",
    "    def autopct_format(pct):\n",
    "        index = np.argmin(np.abs(adjusted_percentages - pct))\n",
    "        return f\"{original_percentages.iloc[index]:.1f}%\"\n",
    "\n",
    "    # Print table\n",
    "    print(\"\\nüîç Memory Tier Summary (In-Scope OS's Only)\")\n",
    "    print(memory_tier_summary)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    explode_values = [0.05] * len(memory_tier_summary)\n",
    "    colors = plt.cm.tab20.colors[:len(memory_tier_summary)]\n",
    "\n",
    "    plt.pie(\n",
    "        adjusted_percentages,\n",
    "        labels=memory_tier_summary.index,\n",
    "        autopct=autopct_format,\n",
    "        startangle=200,\n",
    "        explode=explode_values,\n",
    "        shadow=True,\n",
    "        colors=colors\n",
    "    )\n",
    "    plt.title(\"VM Distribution % by Memory Size Tier\")\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### 5- Group VMs by Disk Size\n",
    "\n",
    "This section categorizes VMs based on their allocated disk size tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e858812-f4c0-4853-8c8e-70f7542e136b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "in_scope_df = pd.read_csv(\"../saved_csv_files/In_Scope_VMs.csv\", low_memory=False)\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns = {'Cleaned OS', 'VM', 'Provisioned MiB'}\n",
    "missing_columns = required_columns - set(in_scope_df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# Convert disk size to TB\n",
    "mib_to_tb_conversion_factor = 2**20 / 10**12\n",
    "in_scope_df['Disk Size TB'] = in_scope_df['Provisioned MiB'] * mib_to_tb_conversion_factor\n",
    "\n",
    "# Identify VMs with missing/zero disk size\n",
    "no_disk_size_vms_df = in_scope_df[(in_scope_df['Disk Size TB'].isna()) | (in_scope_df['Disk Size TB'] == 0)]\n",
    "no_disk_size_vm_count = no_disk_size_vms_df.shape[0]\n",
    "\n",
    "# Filter valid disk size data\n",
    "filtered_disk_df = in_scope_df[in_scope_df['Disk Size TB'] > 0].copy()\n",
    "\n",
    "# Define disk tiers\n",
    "disk_size_bins = [0, 1, 2, 10, 20, 50, 100, float('inf')]\n",
    "disk_bin_labels = ['Tiny (<1 TB)', 'Easy (<=2 TB)', 'Medium (<=10 TB)', 'Hard (<=20 TB)', \n",
    "                   'Very Hard (<=50 TB)', 'White Glove (<=100 TB)', 'Extreme (>100 TB)']\n",
    "\n",
    "filtered_disk_df['Disk Size Tiers'] = pd.cut(\n",
    "    filtered_disk_df['Disk Size TB'],\n",
    "    bins=disk_size_bins,\n",
    "    labels=disk_bin_labels\n",
    ").astype(str)\n",
    "\n",
    "# Create pivot\n",
    "disk_tier_pivot_df = filtered_disk_df.pivot_table(\n",
    "    index='Disk Size Tiers', \n",
    "    values=['VM', 'Disk Size TB'], \n",
    "    aggfunc={'VM': 'count', 'Disk Size TB': 'sum'},\n",
    "    observed=False\n",
    ").reindex(disk_bin_labels).fillna(0)\n",
    "\n",
    "# Check if pivot has expected columns before proceeding\n",
    "if 'VM' not in disk_tier_pivot_df.columns or disk_tier_pivot_df['VM'].sum() == 0:\n",
    "    print(\"‚ö†Ô∏è Warning: No VMs with disk size info. Cannot generate tier summary or pie chart.\")\n",
    "else:\n",
    "    # Round disk sizes\n",
    "    if 'Disk Size TB' in disk_tier_pivot_df.columns:\n",
    "        disk_tier_pivot_df['Disk Size TB'] = disk_tier_pivot_df['Disk Size TB'].round(2)\n",
    "\n",
    "    # Remove tiers with 0 VMs\n",
    "    disk_tier_pivot_df = disk_tier_pivot_df[disk_tier_pivot_df['VM'] > 0]\n",
    "\n",
    "    # Add total row\n",
    "    total_row = pd.DataFrame({\n",
    "        'VM': [disk_tier_pivot_df['VM'].sum()],\n",
    "        'Disk Size TB': [round(disk_tier_pivot_df['Disk Size TB'].sum(), 2)]\n",
    "    }, index=['Total'])\n",
    "    disk_tier_pivot_with_total = pd.concat([disk_tier_pivot_df, total_row])\n",
    "\n",
    "    # Display table\n",
    "    print(\"\\U0001F50D Tier Summary with Total Disk (Formatted):\")\n",
    "    print(disk_tier_pivot_with_total.to_string())\n",
    "\n",
    "    # Calculate original percentages\n",
    "    total_vms = disk_tier_pivot_df['VM'].sum()\n",
    "    original_percentages = (disk_tier_pivot_df['VM'] / total_vms) * 100\n",
    "\n",
    "    # Remove 0% categories\n",
    "    valid_indices = original_percentages > 0\n",
    "    disk_tier_pivot_df = disk_tier_pivot_df[valid_indices]\n",
    "    original_percentages = original_percentages[valid_indices]\n",
    "    disk_bin_labels = disk_tier_pivot_df.index.tolist()\n",
    "\n",
    "    # Adjust small slices\n",
    "    min_percentage = 0.5\n",
    "    new_min_threshold = 1\n",
    "    adjusted_percentages = original_percentages.copy()\n",
    "\n",
    "    below_threshold_mask = adjusted_percentages < min_percentage\n",
    "    num_below_threshold = below_threshold_mask.sum()\n",
    "\n",
    "    if num_below_threshold > 0:\n",
    "        deficit = new_min_threshold * num_below_threshold - adjusted_percentages[below_threshold_mask].sum()\n",
    "        adjusted_percentages[below_threshold_mask] = new_min_threshold\n",
    "        large_slices_mask = ~below_threshold_mask\n",
    "        if large_slices_mask.sum() > 0:\n",
    "            scale_factor = (100 - new_min_threshold * num_below_threshold) / adjusted_percentages[large_slices_mask].sum()\n",
    "            adjusted_percentages[large_slices_mask] *= scale_factor\n",
    "\n",
    "    # Normalize to 100%\n",
    "    adjusted_percentages = adjusted_percentages.round(1)\n",
    "    if len(adjusted_percentages) > 0:\n",
    "        adjusted_percentages.iloc[-1] += 100 - adjusted_percentages.sum()\n",
    "\n",
    "    # Label formatting\n",
    "    def autopct_format(pct):\n",
    "        index = np.argmin(np.abs(adjusted_percentages - pct))\n",
    "        return f\"{original_percentages.iloc[index]:.1f}%\"\n",
    "\n",
    "    # Generate pie chart\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    colors = plt.cm.tab20c(np.linspace(0, 1, len(disk_bin_labels)))\n",
    "    explode_values = [0.05] * len(disk_bin_labels)\n",
    "\n",
    "    plt.pie(\n",
    "        adjusted_percentages,\n",
    "        labels=disk_bin_labels,\n",
    "        autopct=autopct_format,\n",
    "        startangle=200,\n",
    "        explode=explode_values,\n",
    "        shadow=True,\n",
    "        colors=colors\n",
    "    )\n",
    "    plt.title('VM Distribution % by Disk Size Tier')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "# Always show bar chart for missing disk size\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.bar(['No Disk Size'], [no_disk_size_vm_count], color='purple')\n",
    "plt.title('VMs without Disk Size Information')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### 6- Categorize Host Compute Nodes\n",
    "\n",
    "Classify compute nodes across all vCenters based on their model numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Group all hosts by model\n",
    "all_host_model_pivot_df = consolidated_vhost_df.pivot_table(index=['Vendor', 'Model'], values='Host', aggfunc='count')\n",
    "display(all_host_model_pivot_df)\n",
    "\n",
    "# Bar chart for all host models\n",
    "plt.figure(figsize=(12, 8))  # Make it wider to avoid label overlap\n",
    "\n",
    "# Generate a colormap with enough distinct colors\n",
    "norm = mcolors.Normalize(vmin=0, vmax=len(all_host_model_pivot_df)-1)\n",
    "colors = cm.tab20(norm(range(len(all_host_model_pivot_df))))\n",
    "\n",
    "# Create a bar chart with automatic colors\n",
    "bar_plot = all_host_model_pivot_df['Host'].plot(kind='bar', color=colors, edgecolor='black', figsize=(12, 8))\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Host Node Models', fontsize=12)\n",
    "plt.xlabel('Host Model', fontsize=10)\n",
    "plt.ylabel('Host Count', fontsize=10)\n",
    "\n",
    "# Improve x-tick readability by rotating the labels\n",
    "plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "\n",
    "# Add totals on top of each bar\n",
    "for bar in bar_plot.patches:\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "             f'{int(bar.get_height())}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Show the bar chart\n",
    "plt.tight_layout()  # Adjust layout to prevent overlapping labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### 7- Categorize Host Compute Nodes by vCenter\n",
    "\n",
    "Categorize compute nodes by their model number for each vCenter separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Group all the hosts by their model and vCenter\n",
    "all_host_model_vcenter_pivot_df = consolidated_vhost_df.pivot_table(\n",
    "    index=['vCenter', 'Vendor', 'Model'],\n",
    "    values=['Host'],\n",
    "    aggfunc={'Host': 'count'},\n",
    "    observed=False,\n",
    "    margins=False,\n",
    "    sort=True\n",
    ")\n",
    "\n",
    "print(\"üîç \"f'Distribution of ALL host models by vCenter:\\n')\n",
    "print(all_host_model_vcenter_pivot_df)\n",
    "all_host_model_vcenter_pivot_df.to_clipboard(excel=True)\n",
    "\n",
    "# Bar chart creation based on the pivot table (sorted by host count)\n",
    "plt.figure(figsize=(14, 8))  # Increase figure size for better visibility\n",
    "\n",
    "# Prepare labels and data\n",
    "labels = all_host_model_vcenter_pivot_df.index.map(lambda x: f'{x[1]} {x[2]} ({x[0]})')  # Combine Vendor, Model, and vCenter in the label\n",
    "sizes = all_host_model_vcenter_pivot_df['Host']\n",
    "\n",
    "# Automatically assign colors using the 'tab20' colormap\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(sizes)))\n",
    "\n",
    "# Create the vertical bar chart\n",
    "bars = plt.bar(labels, sizes, color=colors, edgecolor='black')\n",
    "\n",
    "# Add text annotations (totals) on top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2, \n",
    "        yval, \n",
    "        int(yval), \n",
    "        ha='center', \n",
    "        va='bottom', \n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "# Add title and formatting\n",
    "plt.title('Host Node Models', fontsize=14)\n",
    "plt.xlabel('Host Model', fontsize=12)\n",
    "plt.ylabel('Number of Hosts', fontsize=12)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Display the bar chart\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### 8- Count the ESXi Datacenters & Clusters\n",
    "\n",
    "Provide a summary of the total number of ESXi datacenters and clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data with low_memory=False to suppress dtype warnings\n",
    "in_scope_df = pd.read_csv(\"../saved_csv_files/In_Scope_VMs.csv\", low_memory=False)\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "required_columns = {'Cleaned OS', 'VM', 'Datacenter', 'Cluster'}\n",
    "missing_columns = required_columns - set(in_scope_df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# Drop rows with NaN in critical columns\n",
    "in_scope_df = in_scope_df.dropna(subset=['Datacenter', 'Cluster', 'VM'])\n",
    "\n",
    "# Filter out rows where 'VM' is an empty string or just whitespace\n",
    "in_scope_df = in_scope_df[in_scope_df['VM'].astype(str).str.strip() != '']\n",
    "\n",
    "# Get unique Datacenters\n",
    "datacenters = in_scope_df['Datacenter'].unique()\n",
    "print(\"üîç \"f'Total VMware Datacenters: {len(datacenters):,}')\n",
    "\n",
    "# Get unique Clusters\n",
    "clusters = in_scope_df['Cluster'].unique()\n",
    "print(\"üîç \"f'Total ESXi clusters: {len(clusters):,}')\n",
    "\n",
    "# Group by Datacenter and count unique Clusters\n",
    "inscope_datacenter_pivot = in_scope_df.groupby('Datacenter')['Cluster'].nunique().reset_index(name='Cluster_Count')\n",
    "inscope_datacenter_pivot.set_index('Datacenter', inplace=True)\n",
    "\n",
    "# Sort by Cluster_Count in descending order\n",
    "inscope_datacenter_pivot = inscope_datacenter_pivot.sort_values(by='Cluster_Count', ascending=False)\n",
    "\n",
    "# Calculate the total number of clusters from the pivot table\n",
    "total_clusters_from_pivot = int(inscope_datacenter_pivot['Cluster_Count'].sum())\n",
    "\n",
    "# Calculate the total number of datacenters\n",
    "total_datacenters = int(inscope_datacenter_pivot.shape[0])\n",
    "\n",
    "# Show only the top 10 datacenters in the summary\n",
    "print(f'\\nüîç Distribution of Clusters to Datacenters (Top 10 by Cluster Count):')\n",
    "print(inscope_datacenter_pivot.head(10).to_string(index=True))\n",
    "print(f'\\nüî• Total Clusters across all Datacenters: {total_clusters_from_pivot}')\n",
    "print(f'üî• Total number of Datacenters: {total_datacenters}')\n",
    "\n",
    "# Debug: Compare Total ESXi clusters vs. Sum of clusters per datacenter\n",
    "if len(clusters) != total_clusters_from_pivot:\n",
    "    print(\"\\n‚ö†Ô∏è Warning: The total ESXi clusters count does not match the sum of clusters across datacenters.\")\n",
    "    print(f\"‚ÄºÔ∏è Total ESXi clusters (unique across dataset): {len(clusters):,}\")\n",
    "    print(f\"‚ÄºÔ∏è Total Clusters from pivot table (sum per datacenter): {total_clusters_from_pivot:,}\")\n",
    "\n",
    "    # Find clusters mapped to multiple datacenters\n",
    "    cluster_datacenter_mapping = in_scope_df.groupby('Cluster')['Datacenter'].nunique()\n",
    "    multi_dc_clusters = cluster_datacenter_mapping[cluster_datacenter_mapping > 1]\n",
    "\n",
    "    if not multi_dc_clusters.empty:\n",
    "        print(\"\\nüîç Clusters that appear in multiple datacenters (Top 10 shown):\")\n",
    "        print(multi_dc_clusters.head(10).to_string(index=True))\n",
    "\n",
    "# Copy full results to clipboard for easy pasting into Excel\n",
    "inscope_datacenter_pivot.to_clipboard(excel=True)\n",
    "\n",
    "# Save the full pivot table to a CSV file\n",
    "csv_filename = \"../saved_csv_files/Cluster_Distribution_by_Datacenter.csv\"\n",
    "inscope_datacenter_pivot.to_csv(csv_filename, index=True)\n",
    "print(f\"\\nüìÇ Data saved to: {csv_filename}\")\n",
    "\n",
    "# Create a pie chart (using only the top 10 datacenters for clarity)\n",
    "top_10_pivot = inscope_datacenter_pivot.head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.pie(top_10_pivot['Cluster_Count'], labels=top_10_pivot.index, \n",
    "        autopct='%1.1f%%', startangle=140, shadow=True, explode=[0.05] * len(top_10_pivot))\n",
    "plt.title('\\nClusters Distribution (Top 10 Datacenters)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### 9- VM distribution by ESXi Clusters\n",
    "\n",
    "This analysis complements the VM distribution by vCenters, as each vCenter typically contains multiple clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data with low_memory=False to suppress dtype warnings\n",
    "in_scope_df = pd.read_csv(\"../saved_csv_files/In_Scope_VMs.csv\", low_memory=False)\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "required_columns = {'Cleaned OS', 'VM', 'Cluster'}\n",
    "missing_columns = required_columns - set(in_scope_df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# Drop rows with missing or empty Cluster/VM names\n",
    "in_scope_df = in_scope_df.dropna(subset=['Cluster', 'VM'])\n",
    "in_scope_df = in_scope_df[in_scope_df['VM'].astype(str).str.strip() != \"\"]\n",
    "\n",
    "# If DataFrame is empty after cleaning, exit early\n",
    "if in_scope_df.empty:\n",
    "    print(\"‚ùå No in-scope VMs found after cleaning. Exiting.\")\n",
    "else:\n",
    "    # Pivot the VM information on the clusters\n",
    "    inscope_cluster_pivot = in_scope_df.pivot_table(index='Cluster',\n",
    "                                                    values=['VM'],  # Ensures it's a DataFrame\n",
    "                                                    aggfunc='count')\n",
    "\n",
    "    # Rename column for clarity, only if 'VM' exists\n",
    "    if 'VM' in inscope_cluster_pivot.columns:\n",
    "        inscope_cluster_pivot.rename(columns={'VM': 'VM_Count'}, inplace=True)\n",
    "    else:\n",
    "        raise KeyError(\"Expected column 'VM' not found in pivot result.\")\n",
    "\n",
    "    # Sort by VM_Count in descending order\n",
    "    inscope_cluster_pivot = inscope_cluster_pivot.sort_values(by='VM_Count', ascending=False)\n",
    "\n",
    "    # Calculate totals\n",
    "    total_vms = int(inscope_cluster_pivot['VM_Count'].sum())\n",
    "    total_clusters = int(inscope_cluster_pivot.shape[0])\n",
    "\n",
    "    # Print summary for top 10 clusters\n",
    "    print(\"\\U0001F50D Distribution of VMs to Clusters (Top 10 by VM Count):\")\n",
    "    print(inscope_cluster_pivot.head(10).to_string(index=True))\n",
    "    print(f'\\n‚ö†Ô∏è VMs that belong to multiple ESXi clusters are counted only once.')\n",
    "    print(f'\\U0001F525 Total VMs across all clusters: {total_vms}')\n",
    "    print(f'\\U0001F525 Total number of clusters: {total_clusters}\\n')\n",
    "\n",
    "    # Save full VM distribution to a CSV file\n",
    "    csv_filename = \"../saved_csv_files/VM_Distribution_to_Clusters.csv\"\n",
    "    inscope_cluster_pivot.to_csv(csv_filename)\n",
    "    print(f'\\U0001F4BE VM distribution saved to {csv_filename}')\n",
    "\n",
    "    # Copy full results to clipboard for easy pasting into Excel\n",
    "    inscope_cluster_pivot.to_clipboard(excel=True)\n",
    "\n",
    "    # Create a pie chart for the top 10 clusters\n",
    "    top_10_pivot = inscope_cluster_pivot.head(10)\n",
    "\n",
    "    if not top_10_pivot.empty:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.pie(top_10_pivot['VM_Count'], labels=top_10_pivot.index,\n",
    "                autopct='%1.1f%%', startangle=140, shadow=True,\n",
    "                explode=[0.05] * len(top_10_pivot))\n",
    "        plt.title('\\n VM Distribution (Top 10 Clusters)')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No data available to plot the pie chart.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### 10- VMs Categorized by Environment\n",
    "1. Create an `Environment` Column (if it doesn't exist): This column is derived from the ESXi cluster name, which indicates the site location.\n",
    "2. Handle Unclassified Clusters: Clusters without a defined environment category will be grouped under `Unknown.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data with low_memory=False to suppress dtype warnings\n",
    "in_scope_df = pd.read_csv(\"../saved_csv_files/In_Scope_VMs.csv\", low_memory=False)\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "required_columns = {'Cleaned OS', 'VM'}\n",
    "missing_columns = required_columns - set(in_scope_df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# Handle missing 'Environment' column\n",
    "if 'Environment' not in in_scope_df.columns:\n",
    "    print(\"‚ö†Ô∏è 'Environment' column missing from in_scope_df. Creating it with default value 'Unknown'.\")\n",
    "    in_scope_df['Environment'] = 'Unknown'\n",
    "\n",
    "# If environment_summary isn't defined, try building it\n",
    "if 'environment_summary' not in globals():\n",
    "    if 'filtered_vinfo_df' not in globals():\n",
    "        raise ValueError(\"Neither environment_summary nor filtered_vinfo_df is defined. Ensure the required data is available.\")\n",
    "\n",
    "    if 'Environment' not in filtered_vinfo_df.columns:\n",
    "        print(\"‚ö†Ô∏è 'Environment' column missing from filtered_vinfo_df. Creating it with default value 'Unknown'.\")\n",
    "        filtered_vinfo_df = filtered_vinfo_df.copy()\n",
    "        filtered_vinfo_df['Environment'] = 'Unknown'\n",
    "\n",
    "    filtered_vinfo_df = filtered_vinfo_df.copy()\n",
    "    filtered_vinfo_df['Environment'] = filtered_vinfo_df['Environment'].fillna('Unknown')\n",
    "    filtered_vinfo_df.loc[filtered_vinfo_df['Environment'].astype(str).str.strip() == '', 'Environment'] = 'Unknown'\n",
    "\n",
    "    environment_summary = (\n",
    "        filtered_vinfo_df.groupby('Environment', as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={'size': 'VM_Count'})\n",
    "    )\n",
    "\n",
    "# Remove 'Total' row if present\n",
    "environment_filtered = environment_summary[environment_summary['Environment'] != 'Total'].copy()\n",
    "\n",
    "# Convert VM_Count to numeric\n",
    "environment_filtered['VM_Count'] = pd.to_numeric(environment_filtered['VM_Count'], errors='coerce')\n",
    "\n",
    "# Check if 'Environment' column exists (should always exist at this point)\n",
    "if 'Environment' not in in_scope_df.columns:\n",
    "    raise KeyError(\"'Environment' column is missing from in_scope_df. Check data source.\")\n",
    "\n",
    "# Filter in-scope VMs only if available\n",
    "if not in_scope_df.empty:\n",
    "    in_scope_vms = in_scope_df[['VM', 'Environment']].dropna(subset=['VM']).drop_duplicates()\n",
    "    environment_filtered = environment_filtered[\n",
    "        environment_filtered['Environment'].isin(in_scope_vms['Environment'].unique())\n",
    "    ].copy()\n",
    "\n",
    "# Plot only if there's data\n",
    "if not environment_filtered.empty:\n",
    "    env_counts = dict(zip(environment_filtered['Environment'], environment_filtered['VM_Count']))\n",
    "    original_percentages = pd.Series(env_counts) / sum(env_counts.values()) * 100\n",
    "\n",
    "    min_percentage = 1\n",
    "    new_min_threshold = 1\n",
    "    adjusted_percentages = original_percentages.copy()\n",
    "    \n",
    "    below_threshold_mask = adjusted_percentages < min_percentage\n",
    "    num_below_threshold = below_threshold_mask.sum()\n",
    "    \n",
    "    if num_below_threshold > 0:\n",
    "        deficit = new_min_threshold * num_below_threshold - adjusted_percentages[below_threshold_mask].sum()\n",
    "        adjusted_percentages.loc[below_threshold_mask] = new_min_threshold\n",
    "        large_slices_mask = ~below_threshold_mask\n",
    "        if large_slices_mask.sum() > 0:\n",
    "            scale_factor = (100 - new_min_threshold * num_below_threshold) / adjusted_percentages[large_slices_mask].sum()\n",
    "            adjusted_percentages.loc[large_slices_mask] *= scale_factor\n",
    "\n",
    "    # Normalize adjusted percentages\n",
    "    adjusted_percentages = adjusted_percentages.round(1)\n",
    "    if adjusted_percentages.sum() != 100:\n",
    "        adjusted_percentages.iloc[-1] += 100 - adjusted_percentages.sum()\n",
    "\n",
    "    def autopct_format(pct):\n",
    "        try:\n",
    "            index = np.argmin(np.abs(adjusted_percentages.values - pct))\n",
    "            return f\"{original_percentages.iloc[index]:.1f}%\"\n",
    "        except IndexError:\n",
    "            return f\"{pct:.1f}%\"\n",
    "\n",
    "    explode_values = [0.05] * len(env_counts)\n",
    "\n",
    "    # Plot pie chart\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.pie(\n",
    "        adjusted_percentages,\n",
    "        labels=environment_filtered['Environment'],\n",
    "        autopct=autopct_format,\n",
    "        startangle=140,\n",
    "        colors=plt.cm.tab20.colors[:len(env_counts)],\n",
    "        shadow=True,\n",
    "        explode=explode_values\n",
    "    )\n",
    "    plt.title('\\n VM Distribution % by Environment')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No matching in-scope environments found. Pie chart will not be generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### 11- Summarize Operating Systems by Supported vs. Unsupported\n",
    "\n",
    "This section provides a breakdown of supported and unsupported operating systems for the in-scope vCenter instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to truncate OS names, ensuring input is always a string\n",
    "def truncate_os_names(series):\n",
    "    return series.rename(lambda x: str(x) if len(str(x)) <= 40 else str(x)[:37] + '...')\n",
    "\n",
    "# Load the data\n",
    "in_scope_df = pd.read_csv(\"../saved_csv_files/In_Scope_VMs.csv\", low_memory=False)\n",
    "out_of_scope_df = pd.read_csv(\"../saved_csv_files/Out_of_Scope_VMs.csv\", low_memory=False)\n",
    "\n",
    "# Drop rows with missing or invalid 'VM' or 'Cleaned OS'\n",
    "in_scope_df = in_scope_df.dropna(subset=['VM', 'Cleaned OS'])\n",
    "out_of_scope_df = out_of_scope_df.dropna(subset=['VM', 'Cleaned OS'])\n",
    "\n",
    "# Filter out rows where 'VM' is 0\n",
    "in_scope_df = in_scope_df[in_scope_df['VM'] != 0]\n",
    "out_of_scope_df = out_of_scope_df[out_of_scope_df['VM'] != 0]\n",
    "\n",
    "# Pie Chart: In-Scope vs Out-of-Scope\n",
    "if not in_scope_df.empty or not out_of_scope_df.empty:\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(\n",
    "        [len(in_scope_df), len(out_of_scope_df)],\n",
    "        labels=[\"In-Scope VM's\", \"Out-of-Scope VM's\"],\n",
    "        autopct='%1.1f%%', shadow=True, startangle=200, explode=(0.1, 0),\n",
    "        colors=['lightgreen', 'lightcoral']\n",
    "    )\n",
    "    plt.title(\"In-Scope vs Out-of-Scope\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\u26a0\\ufe0f No VM data available for Pie Chart.\")\n",
    "\n",
    "# Pivot and Bar Chart: In-Scope\n",
    "if not in_scope_df.empty:\n",
    "    in_scope_os_counts_df = in_scope_df.pivot_table(index='Cleaned OS', values='VM', aggfunc='count')\n",
    "    in_scope_os_counts = in_scope_os_counts_df.iloc[:, 0]  # Convert to Series\n",
    "    in_scope_os_counts.index = truncate_os_names(pd.Series(in_scope_os_counts.index.astype(str))).values\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    bars = in_scope_os_counts.sort_values(ascending=False).plot(kind='bar', color='green')\n",
    "    plt.title('In-Scope OS Counts')\n",
    "    plt.xlabel('Operating System')\n",
    "    plt.ylabel('Number of VMs')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.subplots_adjust(bottom=0.3, top=0.9)\n",
    "\n",
    "    for bar in bars.patches:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{int(bar.get_height())}',\n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\u26a0\\ufe0f No In-Scope VM data available for Bar Chart.\")\n",
    "\n",
    "# Pivot and Bar Chart: Out-of-Scope\n",
    "if not out_of_scope_df.empty:\n",
    "    out_of_scope_os_counts_df = out_of_scope_df.pivot_table(index='Cleaned OS', values='VM', aggfunc='count')\n",
    "    out_of_scope_os_counts = out_of_scope_os_counts_df.iloc[:, 0]  # Convert to Series\n",
    "    out_of_scope_os_counts.index = truncate_os_names(pd.Series(out_of_scope_os_counts.index.astype(str))).values\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    bars = out_of_scope_os_counts.sort_values(ascending=False).plot(kind='bar', color='red')\n",
    "    plt.title('Out-of-Scope OS Counts')\n",
    "    plt.xlabel('Operating System')\n",
    "    plt.ylabel('Number of VMs')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.subplots_adjust(bottom=0.3, top=0.9)\n",
    "\n",
    "    for bar in bars.patches:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{int(bar.get_height())}',\n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\u26a0\\ufe0f No Out-of-Scope VM data available for Bar Chart.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### 12- Migration Complexity by OS\n",
    "\n",
    "Categorized OS‚Äôs into\n",
    "1. Easy\n",
    "2. Medium\n",
    "3. Hard\n",
    "4. White Glove\n",
    "5. Unsupported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Categorize supported OS into difficulty levels\n",
    "os_difficulty_mapping = {\n",
    "    'Red Hat': 'Easy',\n",
    "    'CentOS': 'Medium',\n",
    "    'Windows': 'Medium',\n",
    "    'Ubuntu': 'Hard',\n",
    "    'SUSE Linux Enterprise': 'Hard',\n",
    "    'Oracle': 'White Glove',\n",
    "    'Microsoft SQL': 'White Glove'\n",
    "}\n",
    "\n",
    "# Load the data with low_memory=False to suppress dtype warnings\n",
    "in_scope_df = pd.read_csv(\"../saved_csv_files/In_Scope_VMs.csv\", low_memory=False)\n",
    "\n",
    "# Ensure the dataframe exists and is not empty\n",
    "if 'filtered_vinfo_df' in locals() or 'filtered_vinfo_df' in globals():\n",
    "    if not filtered_vinfo_df.empty:\n",
    "        # Count OS instances from in-scope VMs\n",
    "        in_scope_os_counts = in_scope_df['Cleaned OS'].value_counts()\n",
    "\n",
    "        # Initialize difficulty counts\n",
    "        difficulty_counts = {'Easy': 0, 'Medium': 0, 'Hard': 0, 'White Glove': 0, 'Unsupported': 0}\n",
    "\n",
    "        # Map OS instances to difficulty levels\n",
    "        for os_name, count in in_scope_os_counts.items():\n",
    "            difficulty = next((difficulty for key, difficulty in os_difficulty_mapping.items() if key in os_name), 'Unsupported')\n",
    "            difficulty_counts[difficulty] += count\n",
    "\n",
    "        # Identify unsupported OS instances within in-scope VMs\n",
    "        unsupported_count = difficulty_counts['Unsupported']\n",
    "\n",
    "        # Summary Section\n",
    "        total_os_instances = sum(difficulty_counts.values())\n",
    "\n",
    "        print(\"üîç Migration Complexity Summary\")\n",
    "        print(f\"üßê Total OS Instances Analyzed: {total_os_instances}\")\n",
    "        for level, count in difficulty_counts.items():\n",
    "            print(f\"  {level}: {count}\")\n",
    "        print(f\"\\nüî¥ Unsupported Instances: {unsupported_count}\")\n",
    "\n",
    "        # Print White Glove OS instances\n",
    "        white_glove_os = [os for os in in_scope_os_counts.index if 'Oracle' in os or 'Microsoft SQL' in os]\n",
    "        print(\"\\nüß§ White Glove Instances:\")\n",
    "        for os in white_glove_os:\n",
    "            print(f\"üö® {os}: {in_scope_os_counts[os]}\")\n",
    "\n",
    "        # Filter and print VMs running unsupported OS instances\n",
    "        unsupported_vms = filtered_vinfo_df[filtered_vinfo_df['Cleaned OS'].apply(\n",
    "            lambda x: next((difficulty for key, difficulty in os_difficulty_mapping.items() if key in str(x)), 'Unsupported')\n",
    "        ) == 'Unsupported']\n",
    "\n",
    "        if not unsupported_vms.empty:\n",
    "            print(\"\\nüö® VMs Running Unsupported (but migratable) OS Instances:\")\n",
    "            for vm_name, os_name in zip(unsupported_vms['VM'], unsupported_vms['Cleaned OS']):\n",
    "                print(f\"‚ö†Ô∏è VM: {vm_name}, OS: {os_name}\")\n",
    "\n",
    "        # Create a bar chart with automatic colors\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        difficulty_levels = list(difficulty_counts.keys())\n",
    "        os_counts = list(difficulty_counts.values())\n",
    "\n",
    "        # Generate a colormap automatically\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, len(difficulty_levels)))\n",
    "\n",
    "        bars = plt.bar(difficulty_levels, os_counts, color=colors)\n",
    "        plt.title('\\n Migration Complexity by OS')\n",
    "        plt.xlabel('Difficulty Level')\n",
    "        plt.ylabel('Number of OS Instances')\n",
    "\n",
    "        # Add numeric labels on bars\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, yval, ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è The dataframe 'filtered_vinfo_df' is empty.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è The dataframe 'filtered_vinfo_df' is not defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### 13- Migration Complexity by Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f82ddb2-4864-43d6-9f0b-13f4b4e368a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "in_scope_df = pd.read_csv(\"../saved_csv_files/In_Scope_VMs.csv\", low_memory=False)\n",
    "\n",
    "# Ensure necessary columns exist\n",
    "required_columns = {'VM', 'Provisioned MiB'}\n",
    "missing_columns = required_columns - set(in_scope_df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# Drop rows with NaN or non-numeric values in 'Provisioned MiB'\n",
    "in_scope_df = in_scope_df[pd.to_numeric(in_scope_df['Provisioned MiB'], errors='coerce').notnull()]\n",
    "in_scope_df['Provisioned MiB'] = in_scope_df['Provisioned MiB'].astype(float)\n",
    "\n",
    "# Convert disk size to TB\n",
    "mib_to_tb_conversion_factor = 2**20 / 10**12\n",
    "in_scope_df['Disk Size TB'] = in_scope_df['Provisioned MiB'] * mib_to_tb_conversion_factor\n",
    "\n",
    "# Filter out VMs with zero or negative disk size\n",
    "filtered_vinfo_df = in_scope_df[in_scope_df['Disk Size TB'] > 0].copy()\n",
    "\n",
    "if filtered_vinfo_df.empty:\n",
    "    print(\"‚ö†Ô∏è No VMs with valid disk size > 0 found. Skipping chart generation.\")\n",
    "else:\n",
    "    # Define disk size categories\n",
    "    size_bins = [0, 10, 20, 50, float('inf')]\n",
    "    size_labels = ['Easy (0-10TB)', 'Medium (10-20TB)', 'Hard (20-50TB)', 'White Glove (>50TB)']\n",
    "\n",
    "    # Categorize VMs by disk size\n",
    "    filtered_vinfo_df['Disk Size Category'] = pd.cut(\n",
    "        filtered_vinfo_df['Disk Size TB'], bins=size_bins, labels=size_labels, include_lowest=True\n",
    "    )\n",
    "\n",
    "    # Create summary\n",
    "    disk_size_summary = filtered_vinfo_df['Disk Size Category'].value_counts().reindex(size_labels, fill_value=0).reset_index()\n",
    "    disk_size_summary.columns = ['Disk Size Category', 'VM Count']\n",
    "\n",
    "    # Display summary\n",
    "    print(\"\\U0001F50D Migration Complexity by Disk:\")\n",
    "    print(disk_size_summary.to_string(index=False))\n",
    "\n",
    "    # Generate dynamic colors\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(size_labels)))\n",
    "\n",
    "    # Plot bar chart with dynamic colors\n",
    "    bars = plt.bar(disk_size_summary['Disk Size Category'], disk_size_summary['VM Count'], color=colors)\n",
    "\n",
    "    plt.xlabel('Disk Size Category')\n",
    "    plt.ylabel('Number of VMs')\n",
    "    plt.title('\\n Migration Complexity by Disk Size')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Add numeric labels on bars\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, int(yval), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541e50e-6642-4a7a-a449-6b3dee2f2f69",
   "metadata": {},
   "source": [
    "### 14- Migration Complexity by Disk & OS\n",
    "\n",
    "This section computes disk & os into a complexity matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data safely\n",
    "try:\n",
    "    in_scope_df = pd.read_csv(\"../saved_csv_files/In_Scope_VMs.csv\", low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"CSV file not found. Ensure the file path is correct.\")\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns = {'VM', 'Provisioned MiB', 'Cleaned OS'}\n",
    "missing_columns = required_columns - set(in_scope_df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# Convert disk size to TB\n",
    "mib_to_tb_conversion_factor = 2**20 / 10**12\n",
    "in_scope_df['Disk Size TB'] = in_scope_df['Provisioned MiB'] * mib_to_tb_conversion_factor\n",
    "\n",
    "# Filter out VMs with missing or zero disk size\n",
    "filtered_vinfo_df = in_scope_df[in_scope_df['Disk Size TB'] > 0].copy()\n",
    "\n",
    "# Define disk size categories\n",
    "size_bins = [0, 10, 20, 50, float('inf')]\n",
    "size_labels = ['Easy (0-10TB)', 'Medium (10-20TB)', 'Hard (20-50TB)', 'White Glove (>50TB)']\n",
    "filtered_vinfo_df['Disk Size Category'] = pd.cut(\n",
    "    filtered_vinfo_df['Disk Size TB'], bins=size_bins, labels=size_labels, include_lowest=True\n",
    ")\n",
    "\n",
    "# OS difficulty mapping\n",
    "os_difficulty_mapping = {\n",
    "    'Red Hat': 'Easy',\n",
    "    'CentOS': 'Medium',\n",
    "    'Windows': 'Medium',\n",
    "    'Ubuntu': 'Hard',\n",
    "    'SUSE Linux Enterprise': 'Hard',\n",
    "    'Oracle': 'Database',\n",
    "    'Microsoft SQL': 'Database'\n",
    "}\n",
    "\n",
    "# Map OS to difficulty level safely\n",
    "filtered_vinfo_df['OS Difficulty'] = filtered_vinfo_df['Cleaned OS'].astype(str).apply(\n",
    "    lambda os: next((difficulty for key, difficulty in os_difficulty_mapping.items() if key in os), 'Unsupported')\n",
    ")\n",
    "\n",
    "# Define complexity mapping function\n",
    "def determine_complexity(row):\n",
    "    os_difficulty = row['OS Difficulty']\n",
    "    disk_category = row['Disk Size Category']\n",
    "    \n",
    "    if os_difficulty == 'Database':\n",
    "        return 'White Glove'\n",
    "    \n",
    "    if os_difficulty == 'Easy':\n",
    "        return 'Easy' if disk_category in ['Easy (0-10TB)', 'Medium (10-20TB)'] else 'Hard'\n",
    "    \n",
    "    if os_difficulty == 'Medium':\n",
    "        return 'Medium' if disk_category in ['Easy (0-10TB)', 'Medium (10-20TB)'] else 'Hard'\n",
    "    \n",
    "    if os_difficulty == 'Hard':\n",
    "        return 'Medium' if disk_category in ['Easy (0-10TB)', 'Medium (10-20TB)'] else 'Hard'\n",
    "    \n",
    "    return 'Unsupported'\n",
    "\n",
    "# Apply complexity mapping\n",
    "filtered_vinfo_df['Migration Complexity'] = filtered_vinfo_df.apply(determine_complexity, axis=1)\n",
    "\n",
    "# Create summary table counting VMs per complexity level\n",
    "complexity_order = ['Easy', 'Medium', 'Hard', 'White Glove', 'Unsupported']\n",
    "complexity_summary = filtered_vinfo_df['Migration Complexity'].value_counts().reindex(complexity_order, fill_value=0).reset_index()\n",
    "complexity_summary.columns = ['Migration Complexity', 'VM Count']\n",
    "complexity_summary = complexity_summary[complexity_summary['VM Count'] > 0]\n",
    "\n",
    "# Display complexity summary\n",
    "print(\"\\U0001F50D Migration Summary:\")\n",
    "print(complexity_summary.to_string(index=False, header=True))\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(complexity_summary['Migration Complexity'], complexity_summary['VM Count'], color=plt.cm.tab10.colors)\n",
    "plt.xlabel('Migration Complexity')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('Migration Complexity Distribution')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate percentage for pie chart\n",
    "complexity_summary = complexity_summary.copy()\n",
    "complexity_summary['Percentage'] = (complexity_summary['VM Count'] / complexity_summary['VM Count'].sum()) * 100\n",
    "\n",
    "# Normalize small percentages\n",
    "def adjust_percentages(percentages, min_threshold=0.5, new_min=1):\n",
    "    percentages = percentages.copy()\n",
    "    below_threshold = percentages < min_threshold\n",
    "    num_below = below_threshold.sum()\n",
    "    \n",
    "    if num_below > 0:\n",
    "        deficit = new_min * num_below - percentages[below_threshold].sum()\n",
    "        percentages.loc[below_threshold] = new_min\n",
    "        large_mask = ~below_threshold\n",
    "        percentages.loc[large_mask] *= (100 - new_min * num_below) / percentages.loc[large_mask].sum()\n",
    "    \n",
    "    return percentages.round(1)\n",
    "\n",
    "complexity_summary['Adjusted Percentage'] = adjust_percentages(complexity_summary['Percentage'])\n",
    "\n",
    "# Filter out zero percentages\n",
    "valid_indices = complexity_summary['Adjusted Percentage'] > 0\n",
    "adjusted_percentages = complexity_summary.loc[valid_indices, 'Adjusted Percentage']\n",
    "valid_labels = complexity_summary.loc[valid_indices, 'Migration Complexity']\n",
    "\n",
    "# Plot pie chart\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "explode = [0.05 if p > 0 else 0 for p in adjusted_percentages]\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    adjusted_percentages, labels=valid_labels,\n",
    "    autopct=lambda pct: f\"{pct:.1f}%\" if pct > 0 else '',\n",
    "    colors=plt.cm.tab20.colors,\n",
    "    startangle=200, explode=explode, shadow=True\n",
    ")\n",
    "\n",
    "for text in texts + autotexts:\n",
    "    text.set_fontsize(8)\n",
    "    text.set_weight('normal')\n",
    "\n",
    "plt.title('Migration Complexity Distribution (Percentage)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### 15- Estimated Migration Time by OS\n",
    "\n",
    "`fte_count` can be adjusted to reflect the number of engineers assigned to deliver the engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Constants\n",
    "migration_time_per_500gb = 110   # minutes (1 hour 50 minutes per 500GB)\n",
    "te_hours_per_day = 8             # 8 hours per day per FTE\n",
    "fte_count = 10                   # 10 FTEs available\n",
    "pmt_hours = 1                    # Post-migration troubleshooting time per VM in hours\n",
    "\n",
    "# Check if filtered_vinfo_df is defined\n",
    "try:\n",
    "    filtered_vinfo_df\n",
    "except NameError:\n",
    "    raise ValueError(\"filtered_vinfo_df is not defined. Ensure it is loaded before running this script.\")\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = {'Cleaned OS', 'Disk Size TB', 'VM', 'Cluster'}\n",
    "missing_columns = required_columns - set(filtered_vinfo_df.columns)\n",
    "\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following required columns are missing in filtered_vinfo_df: {missing_columns}\")\n",
    "\n",
    "# Copy and preprocess DataFrame\n",
    "inscope_vinfo_df = filtered_vinfo_df.copy()\n",
    "inscope_vinfo_df['Cleaned OS'] = inscope_vinfo_df['Cleaned OS'].fillna(\"Unknown OS\").astype(str).str.strip()\n",
    "inscope_vinfo_df['Disk Size TB'] = pd.to_numeric(inscope_vinfo_df['Disk Size TB'], errors='coerce').fillna(0)\n",
    "inscope_vinfo_df['VM'] = inscope_vinfo_df['VM'].fillna('Unknown VM')\n",
    "inscope_vinfo_df['Cluster'] = inscope_vinfo_df['Cluster'].fillna('').str.lower()\n",
    "\n",
    "# Define disk size classification\n",
    "size_bins = [0, 10, 20, 50, float('inf')]\n",
    "size_labels = ['Easy (0-10TB)', 'Medium (10-20TB)', 'Hard (20-50TB)', 'White Glove (>50TB)']\n",
    "inscope_vinfo_df['Disk Size Category'] = pd.cut(inscope_vinfo_df['Disk Size TB'], bins=size_bins, labels=size_labels)\n",
    "\n",
    "# Generate supported OS list dynamically\n",
    "supported_os_counts = filtered_vinfo_df['Cleaned OS'].value_counts().index.tolist()\n",
    "inscope_vinfo_df['OS Support'] = inscope_vinfo_df['Cleaned OS'].apply(\n",
    "    lambda os: 'Supported' if any(supported_os in os for supported_os in supported_os_counts) else 'Not Supported'\n",
    ")\n",
    "\n",
    "# Complexity classification\n",
    "def classify_complexity(row):\n",
    "    cluster = str(row.get('Cluster', '')).lower()\n",
    "    disk_size_category = row.get('Disk Size Category', '')\n",
    "\n",
    "    if cluster.startswith('sql-') or 'oracle' in cluster:\n",
    "        return 'White Glove'\n",
    "\n",
    "    complexity_map = {\n",
    "        'Easy (0-10TB)': 'Easy',\n",
    "        'Medium (10-20TB)': 'Medium',\n",
    "        'Hard (20-50TB)': 'Hard',\n",
    "        'White Glove (>50TB)': 'White Glove'\n",
    "    }\n",
    "    return complexity_map.get(disk_size_category, 'Unknown')\n",
    "\n",
    "inscope_vinfo_df['Complexity'] = inscope_vinfo_df.apply(classify_complexity, axis=1)\n",
    "\n",
    "# Sort complexity order\n",
    "complexity_order = ['Easy', 'Medium', 'Hard', 'White Glove', 'Database']\n",
    "inscope_vinfo_df['Complexity'] = pd.Categorical(inscope_vinfo_df['Complexity'], categories=complexity_order, ordered=True)\n",
    "inscope_vinfo_df = inscope_vinfo_df.sort_values('Complexity')\n",
    "\n",
    "# Compute Migration Time\n",
    "inscope_vinfo_df['Migration Time (minutes)'] = inscope_vinfo_df['Disk Size TB'].apply(\n",
    "    lambda size: ((size * 1024) / 500) * migration_time_per_500gb\n",
    ")\n",
    "\n",
    "# Compute Total Time (Migration + Post-Migration)\n",
    "pmt_minutes = pmt_hours * 60\n",
    "inscope_vinfo_df['Total Time (minutes)'] = inscope_vinfo_df['Migration Time (minutes)'] + pmt_minutes\n",
    "\n",
    "# Summarize data\n",
    "disk_classification_summary = inscope_vinfo_df.groupby(['Complexity', 'OS Support'], observed=True).agg(\n",
    "    VM_Count=('VM', 'count'),\n",
    "    Total_Disk=('Disk Size TB', 'sum'),\n",
    "    Total_Mig_Time_Minutes=('Total Time (minutes)', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Compute Total Days and Weeks before formatting\n",
    "disk_classification_summary['Total_Days'] = disk_classification_summary['Total_Mig_Time_Minutes'] / (te_hours_per_day * 60 * fte_count)\n",
    "disk_classification_summary['Total_Weeks'] = disk_classification_summary['Total_Days'] / 5\n",
    "\n",
    "# Add totals row before formatting\n",
    "totals_row = {\n",
    "    'Complexity': 'Total',\n",
    "    'OS Support': '',\n",
    "    'VM_Count': disk_classification_summary['VM_Count'].sum(),\n",
    "    'Total_Disk': disk_classification_summary['Total_Disk'].sum(),\n",
    "    'Total_Mig_Time_Minutes': disk_classification_summary['Total_Mig_Time_Minutes'].sum(),\n",
    "    'Total_Days': disk_classification_summary['Total_Days'].sum(),\n",
    "    'Total_Weeks': disk_classification_summary['Total_Weeks'].sum()\n",
    "}\n",
    "disk_classification_summary = pd.concat([disk_classification_summary, pd.DataFrame([totals_row])], ignore_index=True)\n",
    "\n",
    "# Format numeric columns for display\n",
    "disk_classification_summary['Total_Disk'] = disk_classification_summary['Total_Disk'].apply(lambda x: f\"{x:.2f} TB\")\n",
    "disk_classification_summary['Total_Days'] = disk_classification_summary['Total_Days'].apply(lambda x: f\"{x:.1f}\")\n",
    "disk_classification_summary['Total_Weeks'] = disk_classification_summary['Total_Weeks'].apply(lambda x: f\"{x:.1f}\")\n",
    "\n",
    "# Drop intermediate column\n",
    "disk_classification_summary = disk_classification_summary.drop(columns=['Total_Mig_Time_Minutes'])\n",
    "\n",
    "# Function to format the summary table\n",
    "def format_table(headers, rows):\n",
    "    horizontal_line = \"‚îÄ\"\n",
    "    vertical_line = \"‚îÇ\"\n",
    "    corner_tl, corner_tr = \"‚ï≠\", \"‚ïÆ\"\n",
    "    corner_bl, corner_br = \"‚ï∞\", \"‚ïØ\"\n",
    "    join_t, join_b, join_c = \"‚î¨\", \"‚î¥\", \"‚îº\"\n",
    "    col_widths = [max(len(str(item)) for item in col) for col in zip(headers, *rows)]\n",
    "\n",
    "    def make_row(items):\n",
    "        return vertical_line + vertical_line.join(f\"{str(item).rjust(width)}\" for item, width in zip(items, col_widths)) + vertical_line\n",
    "\n",
    "    top_line = corner_tl + join_t.join(horizontal_line * width for width in col_widths) + corner_tr\n",
    "    header_row = make_row(headers)\n",
    "    divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"‚îú\", \"‚î§\"])\n",
    "    data_rows = [make_row(row) for row in rows[:-1]]\n",
    "    totals_divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"‚îú\", \"‚î§\"])\n",
    "    totals_row = make_row(rows[-1])\n",
    "    bottom_line = corner_bl + join_b.join(horizontal_line * width for width in col_widths) + corner_br\n",
    "    return \"\\n\".join([top_line, header_row, divider_row] + data_rows + [totals_divider_row, totals_row, bottom_line])\n",
    "\n",
    "# Convert DataFrame to formatted table\n",
    "headers = list(disk_classification_summary.columns)\n",
    "rows = disk_classification_summary.values.tolist()\n",
    "formatted_table = format_table(headers, rows)\n",
    "\n",
    "# Print formatted summary table\n",
    "print(f\"üîç Migration Summary\")\n",
    "print(formatted_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a5090-41a4-48c2-8f84-2afec0cf6089",
   "metadata": {},
   "source": [
    "### 16- Estimated Migration Time by vCenter\n",
    "\n",
    "`fte_count` can be adjusted to reflect the number of engineers assigned to deliver the engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334fa83-e0f0-4c23-b4df-af8f97541f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "fte_hours_per_day = 8  # 8 hours per day per FTE\n",
    "fte_count = 10         # 10 FTEs available\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns = {'Complexity', 'OS Support', 'VM', 'Disk Size TB', 'Total Time (minutes)', 'vCenter'}\n",
    "missing_columns = required_columns - set(inscope_vinfo_df.columns)\n",
    "\n",
    "# Handle missing columns\n",
    "if 'OS Support' in missing_columns:\n",
    "    inscope_vinfo_df['OS Support'] = 'Unknown'  # Defaulting to 'Unknown'\n",
    "\n",
    "# If 'Complexity' is missing, classify based on disk size or other available data\n",
    "if 'Complexity' in missing_columns:\n",
    "    # Define disk size classification\n",
    "    size_bins = [0, 10, 20, 50, float('inf')]\n",
    "    size_labels = ['Easy', 'Medium', 'Hard', 'White Glove']\n",
    "    inscope_vinfo_df['Disk Size Category'] = pd.cut(inscope_vinfo_df['Disk Size TB'], bins=size_bins, labels=size_labels)\n",
    "\n",
    "    # Define complexity based on disk size and vCenter name\n",
    "    def classify_complexity(row):\n",
    "        cluster = str(row.get('vCenter', '')).lower()\n",
    "        disk_size_category = row.get('Disk Size Category', '')\n",
    "\n",
    "        if cluster.startswith('sql-') or 'oracle' in cluster:\n",
    "          return 'White Glove'\n",
    "\n",
    "        complexity_map = {\n",
    "            'Easy': 'Easy',\n",
    "            'Medium': 'Medium',\n",
    "            'Hard': 'Hard',\n",
    "            'White Glove': 'White Glove'\n",
    "        }\n",
    "        return complexity_map.get(disk_size_category, 'Unknown')  # Default to 'Tiny'\n",
    "\n",
    "    inscope_vinfo_df['Complexity'] = inscope_vinfo_df.apply(classify_complexity, axis=1)\n",
    "\n",
    "# Recheck if any required columns are missing after handling defaults\n",
    "missing_columns = required_columns - set(inscope_vinfo_df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following required columns are missing in inscope_vinfo_df: {missing_columns}\")\n",
    "\n",
    "# Define complexity order\n",
    "complexity_order = ['Easy', 'Medium', 'Hard', 'White Glove']\n",
    "inscope_vinfo_df['Complexity'] = pd.Categorical(\n",
    "    inscope_vinfo_df['Complexity'],\n",
    "    categories=complexity_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Get unique vCenters\n",
    "vcenters = inscope_vinfo_df['vCenter'].unique()\n",
    "\n",
    "# Dictionary to store summaries for each vCenter\n",
    "vcenter_summaries = {}\n",
    "\n",
    "# Function to format the table output\n",
    "def format_table(headers, rows):\n",
    "    horizontal_line = \"‚îÄ\"\n",
    "    vertical_line = \"‚îÇ\"\n",
    "    corner_tl, corner_tr = \"‚ï≠\", \"‚ïÆ\"\n",
    "    corner_bl, corner_br = \"‚ï∞\", \"‚ïØ\"\n",
    "    join_t, join_b, join_c = \"‚î¨\", \"‚î¥\", \"‚îº\"\n",
    "    col_widths = [max(len(str(item)) for item in col) for col in zip(headers, *rows)]\n",
    "    \n",
    "    def make_row(items):\n",
    "        return vertical_line + vertical_line.join(f\"{str(item).rjust(width)}\" for item, width in zip(items, col_widths)) + vertical_line\n",
    "    \n",
    "    top_line = corner_tl + join_t.join(horizontal_line * width for width in col_widths) + corner_tr\n",
    "    header_row = make_row(headers)\n",
    "    divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"‚îú\", \"‚î§\"])\n",
    "    data_rows = [make_row(row) for row in rows[:-1]]\n",
    "    totals_divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"‚îú\", \"‚î§\"])\n",
    "    totals_row = make_row(rows[-1])\n",
    "    bottom_line = corner_bl + join_b.join(horizontal_line * width for width in col_widths) + corner_br\n",
    "    return \"\\n\".join([top_line, header_row, divider_row] + data_rows + [totals_divider_row, totals_row, bottom_line])\n",
    "\n",
    "# Process each vCenter separately\n",
    "for vcenter in vcenters:\n",
    "    vcenter_df = inscope_vinfo_df[inscope_vinfo_df['vCenter'] == vcenter]\n",
    "    disk_classification_summary = vcenter_df.groupby(['Complexity', 'OS Support'], observed=True).agg(\n",
    "        VM_Count=('VM', 'count'),\n",
    "        Total_Disk=('Disk Size TB', 'sum'),\n",
    "        Total_Mig_Time_Minutes=('Total Time (minutes)', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Ensure all categories exist\n",
    "    for complexity in complexity_order:\n",
    "        for os_support in ['Supported', 'Not Supported', 'Unknown']:\n",
    "            if not ((disk_classification_summary['Complexity'] == complexity) &\n",
    "                    (disk_classification_summary['OS Support'] == os_support)).any():\n",
    "                disk_classification_summary = pd.concat([\n",
    "                    disk_classification_summary,\n",
    "                    pd.DataFrame({\n",
    "                        'Complexity': [complexity],\n",
    "                        'OS Support': [os_support],\n",
    "                        'VM_Count': [0],\n",
    "                        'Total_Disk': [0.0],\n",
    "                        'Total_Mig_Time_Minutes': [0]\n",
    "                    })\n",
    "                ], ignore_index=True)\n",
    "    \n",
    "    # Remove rows where VM_Count is zero\n",
    "    disk_classification_summary = disk_classification_summary[disk_classification_summary['VM_Count'] > 0]\n",
    "    \n",
    "    # Set complexity order\n",
    "    disk_classification_summary['Complexity'] = pd.Categorical(\n",
    "        disk_classification_summary['Complexity'], categories=complexity_order, ordered=True\n",
    "    )\n",
    "    disk_classification_summary = disk_classification_summary.sort_values('Complexity')\n",
    "\n",
    "    # Compute total migration time and disk\n",
    "    total_disk_tb_numeric = disk_classification_summary['Total_Disk'].astype(float).sum()\n",
    "    total_mig_time_minutes = disk_classification_summary['Total_Mig_Time_Minutes'].sum()\n",
    "\n",
    "    # Format columns\n",
    "    disk_classification_summary['Formatted_Mig_Time'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "        lambda minutes: f\"{minutes / 60:,.1f}h\"\n",
    "    )\n",
    "    disk_classification_summary['Days_Per_FTEs'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "        lambda minutes: f\"{minutes / (fte_hours_per_day * 60 * fte_count):,.1f}\"\n",
    "    )\n",
    "\n",
    "    disk_classification_summary['VM_Count'] = disk_classification_summary['VM_Count'].astype(int).apply(lambda x: f\"{x:,}\")\n",
    "    disk_classification_summary['Total_Disk'] = disk_classification_summary['Total_Disk'].astype(float).apply(lambda x: f\"{x:,.0f}\")\n",
    "\n",
    "    # Add totals row\n",
    "    totals_row = {\n",
    "        'Complexity': 'Totals',\n",
    "        'OS Support': '',\n",
    "        'VM_Count': f\"{disk_classification_summary['VM_Count'].astype(str).replace(',', '', regex=True).astype(int).sum():,}\",\n",
    "        'Total_Disk': f\"{total_disk_tb_numeric:,.0f}\",\n",
    "        'Formatted_Mig_Time': f\"{total_mig_time_minutes / 60:,.1f}h\",\n",
    "        'Days_Per_FTEs': f\"{total_mig_time_minutes / (fte_hours_per_day * 60 * fte_count):,.1f}\"\n",
    "    }\n",
    "    disk_classification_summary = pd.concat([\n",
    "        disk_classification_summary, pd.DataFrame([totals_row])\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    vcenter_summaries[vcenter] = disk_classification_summary\n",
    "    print(f\"\\nüîç Migration Summary for vCenter: {vcenter}\")\n",
    "    headers = [\"Complexity\", \"OS Support\", \"VM Count\", \"Total Disk (TB)\", \"Total Migration Time\", \"Total Days\"]\n",
    "    rows = disk_classification_summary[['Complexity', 'OS Support', 'VM_Count', 'Total_Disk', 'Formatted_Mig_Time', 'Days_Per_FTEs']].values.tolist()\n",
    "    print(format_table(headers, rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc2e3b-eafe-4dc0-a4b8-734fe68b2ae5",
   "metadata": {},
   "source": [
    "### 17- Estimated Migration Time by Environment\n",
    "\n",
    "`fte_count` can be adjusted to reflect the number of engineers assigned to deliver the engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f64331-b58c-477a-8a66-9b76f7011535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# Constants\n",
    "MIGRATION_TIME_PER_500GB = 110   # minutes (1 hour 50 minutes per 500GB)\n",
    "TE_HOURS_PER_DAY = 8             # 8 hours per day per FTE\n",
    "FTE_COUNT = 10                   # 10 FTEs available\n",
    "PMT_HOURS = 1                    # Post-migration troubleshooting time per VM in hours\n",
    "PMT_MINUTES = PMT_HOURS * 60\n",
    "\n",
    "# Validate input DataFrame\n",
    "try:\n",
    "    filtered_vinfo_df\n",
    "except NameError:\n",
    "    raise ValueError(\"filtered_vinfo_df is not defined. Ensure it is loaded before running this script.\")\n",
    "\n",
    "# Normalize column names\n",
    "filtered_vinfo_df.columns = filtered_vinfo_df.columns.str.strip()\n",
    "\n",
    "# Required columns\n",
    "REQUIRED_COLUMNS = {'Environment', 'Disk Size TB', 'VM'}\n",
    "missing_columns = REQUIRED_COLUMNS - set(filtered_vinfo_df.columns)\n",
    "\n",
    "# Handle missing \"Environment\" column\n",
    "if 'Environment' in missing_columns:\n",
    "    print(\"‚ö†Ô∏è 'Environment' column is missing. Creating it with default value 'Unknown'.\")\n",
    "    filtered_vinfo_df['Environment'] = 'Unknown'\n",
    "    missing_columns.remove('Environment')\n",
    "\n",
    "# If other required columns are still missing, raise an error\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# Preprocess DataFrame\n",
    "inscope_vinfo_df = filtered_vinfo_df.copy()\n",
    "inscope_vinfo_df['Environment'] = inscope_vinfo_df['Environment'].fillna('Unknown').astype(str).str.strip()\n",
    "inscope_vinfo_df['VM'] = inscope_vinfo_df['VM'].fillna('Unknown VM')\n",
    "inscope_vinfo_df['Disk Size TB'] = pd.to_numeric(inscope_vinfo_df['Disk Size TB'], errors='coerce').fillna(0)\n",
    "\n",
    "# Compute Migration and Total Time\n",
    "inscope_vinfo_df['Total Time (minutes)'] = inscope_vinfo_df['Disk Size TB'].apply(\n",
    "    lambda size: ((size * 1024) / 500) * MIGRATION_TIME_PER_500GB + PMT_MINUTES\n",
    ")\n",
    "\n",
    "# Summarize data\n",
    "environment_summary = inscope_vinfo_df.groupby('Environment', observed=True).agg(\n",
    "    VM_Count=('VM', 'count'),\n",
    "    Total_Disk_TB=('Disk Size TB', 'sum'),\n",
    "    Total_Time_Minutes=('Total Time (minutes)', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Compute Total Days and Weeks\n",
    "environment_summary['Total_Days'] = environment_summary['Total_Time_Minutes'] / (TE_HOURS_PER_DAY * 60 * FTE_COUNT)\n",
    "environment_summary['Total_Weeks'] = environment_summary['Total_Days'] / 5\n",
    "\n",
    "# Add totals row (replace None with np.nan to avoid warning)\n",
    "totals_row = {\n",
    "    'Environment': 'Total',\n",
    "    'VM_Count': environment_summary['VM_Count'].sum(),\n",
    "    'Total_Disk_TB': environment_summary['Total_Disk_TB'].sum(),\n",
    "    'Total_Time_Minutes': environment_summary['Total_Time_Minutes'].sum(),\n",
    "    'Total_Days': environment_summary['Total_Days'].sum(),\n",
    "    'Total_Weeks': environment_summary['Total_Weeks'].sum()\n",
    "}\n",
    "\n",
    "totals_df = pd.DataFrame([totals_row])\n",
    "environment_summary = pd.concat([environment_summary, totals_df], ignore_index=True)\n",
    "\n",
    "# Format columns for display\n",
    "environment_summary['Total_Disk_TB'] = environment_summary['Total_Disk_TB'].map(lambda x: f\"{x:.2f} TB\" if pd.notnull(x) else \"\")\n",
    "environment_summary['Total_Days'] = environment_summary['Total_Days'].map(lambda x: f\"{x:.1f}\" if pd.notnull(x) else \"\")\n",
    "environment_summary['Total_Weeks'] = environment_summary['Total_Weeks'].map(lambda x: f\"{x:.1f}\" if pd.notnull(x) else \"\")\n",
    "\n",
    "# Drop time in minutes for display purposes\n",
    "environment_summary.drop(columns=['Total_Time_Minutes'], inplace=True)\n",
    "\n",
    "# Display summary\n",
    "display(environment_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977fd21-5e69-4260-9664-1d0d99b9ebb9",
   "metadata": {},
   "source": [
    "### 18- Estimated Migration Time (Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9bb6a-587e-4c7b-898d-098d91cbea79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate global totals across all vCenters\n",
    "global_total_mig_time = sum(\n",
    "    summary['Total_Mig_Time_Minutes'].sum() for summary in vcenter_summaries.values()\n",
    ")\n",
    "global_total_days = global_total_mig_time / (fte_hours_per_day * 60 * fte_count)\n",
    "\n",
    "# Calculate total weeks (assuming each week has 5 workdays)\n",
    "total_global_weeks = global_total_days / 5\n",
    "\n",
    "# Ensure all values are correctly formatted\n",
    "print(f\"‚úÖÔ∏è Global Total Migration Time: {global_total_mig_time / 60:,.1f}h\")\n",
    "print(f\"‚úÖÔ∏è Global Total Days: {global_total_days:,.1f}\")\n",
    "print(f\"‚úÖÔ∏è Global Total Weeks: {total_global_weeks:,.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
